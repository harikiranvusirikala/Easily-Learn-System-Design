Structure of Notes:
===
1. Introduction:
System Design Interviews
Introduction
Abstractions
Non-functional system characteristics
Back-of-the-envelope calculations


Section: System Design interviews:
=====
System Design interviews are open to discussion and involve multiple possible solutions that can be re-iterated.

ability to approach a problem, think critically, and make trade-offs.
understanding the problem, breaking it down, and finding the most optimal solution.

figure out the requirements and map them onto the computational components and the high-level communication protocols that connect these subsystems.

The final answer doesn’t matter. What matters is the process and the journey that a good applicant takes the interviewer through.

Best practices:
ask the right questions to solidify the requirements
need to scope the problem
engage the interviewer

how a design might evolve over time as some aspect of the system increases by some order of magnitude

There’s no single correct approach or solution to a design problem.
A lot is predicated on the assumptions we make.

our responsibility: provide fault tolerance at the design level

Theory: mostly from distributed systems - https://www.educative.io/courses/distributed-systems-practitioners

• Robustness (the ability to maintain operations during a crisis)
• Scalability
• Availability
• Performance
• Extensibility
• Resiliency (the ability to return to normal operations over an acceptable period of time post-disruption)

interviewer evaluates the candidate’s ability to discuss the different aspects of the system and assess the solution based on the requirements that might evolve during the conversation.



Key Concepts to Prepare:
=====
1. Fundamental concepts in System Design interview:
===
CAP theorem - availability or consistency under-network partitions when network components fail.

• PACELC theorem: CAP theorem and without partitions, latency or consistency.

• If Network (P)artitioning happens then
	• Choose from (A)vailability Or (C)onsistency
• (E)lse
	• Choose from (L)atency Or (C)onsistency

PC/EC system include BigTable and HBase
PA/EL system include Dynamo and Cassandra
PA/EC system include MongoDB

• Heartbeat: all servers periodically send a heartbeat message

• AJAX polling/HTTP short-polling: client request, server respone. Empty response on no data.

• HTTP long-polling/hanging GET: hold till data available instead of empty, client immediately re-requests information.

• WebSockets: connection through WebSocket handshake. server & client bidrectional data exchange.

• Server-sent events (SSEs): long-term connection, server send data to client, but client required the use another technology or protocol to send data.


2. Fundamentals of distributed system:
===
• Data durability and consistency: We must understand the differences and impacts of storage solution failure and corruption rates in read-write processes.

• Replication: key to unlocking data durability and consistency. It deals with backing up data but also with repeating processes at scale.

• Partitioning/sharding: partitions divide data across different nodes within our system. As replication distributes data across nodes, partitioning distributes processes across nodes, reducing the reliance on pure replication.

• Consensus: Consider each node in different location of world, can all be synchronized. This is consensus.
all the nodes need to agree, which will prevent faulty processes from running and ensure consistency and replication of data and processes across the system.

• Distributed transactions: Once we’ve achieved consensus, now transactions from applications need to be committed across databases, with fault checks performed by each involved resource. Two-way and three-way communication to read, write, and commit are shared across participant nodes.


3. The architecture of large-scale web applications:
===
• N-tier applications: processing happens in different layers(called tiers) some on client, server, and other server in one application.
Understanding how these tiers interact and processes they are responsible is part of System Design for the web.

• HTTP and REST: HTTP is the sole API on which the entire internet runs.
REST is a set of design principles to directly interact with the API that is HTTP, allowing efficient, scalable systems with components isolated from each other’s assumptions.

• DNS and load balancing: Helps in not making a server overload, while other is idle.
Routing client requests to the right server, the right tier where processing happens, helps ensure system stability.

• Caching: A cache makes our most frequently requested data and applications accessible to most users at high speeds.
The questions for our web application are what needs to be stored in the cache, how we direct traffic to the cache, and what happens when we don’t have what we want in the cache.

• Stream processing: Stream processing applies uniform processes to the data stream.
If an application has continuous, consistent data passing through it, then stream processing allows efficient use of local resources within the application.


4. Design of large-scale distributed systems:
===
Once we know the basics of "distributed systems" and "web architecture", it is time to apply this learning and design real-world systems.
• https://www.educative.io/courses/distributed-systems-practitioners
• https://www.educative.io/courses/software-architecture-in-applications

Finding and optimizing potential solutions to these problems will give us the tools to approach the System Design interview with confidence.
=====


Resources:
===
This course
Technical blogs/System Design interview articles
We can study these blogs to gain insight into the company’s challenges or problems and the changes it made in the design to cope with them.

https://engineering.fb.com/
https://research.fb.com/
https://aws.amazon.com/blogs/architecture/
https://www.amazon.science/blog
https://netflixtechblog.com/
https://research.google/
https://quoraengineering.quora.com/
https://eng.uber.com/
https://databricks.com/blog/category/engineering
https://medium.com/@Pinterest_Engineering
https://medium.com/blackrock-engineering
https://eng.lyft.com/
https://engineering.salesforce.com/

https://www.educative.io/blog/google-system-design-interview-questions
https://www.educative.io/blog/microsoft-system-design-interview
https://www.educative.io/blog/netflix-system-design-interview-questions
https://www.educative.io/blog/amazon-system-design-interview
https://www.educative.io/blog/meta-system-design-interview

https://www.educative.io/blog/category/system-design
https://www.educative.io/blog/system-design-interview-questions

We should start with high-level stuff because the low-level details will automatically come up.

https://www.educative.io/mock-interview


Do's: Strategize, then divide and conquer:
===
1• Ask refining questions
prioritize the main features by asking the interviewer refining questions
	• clients direct requirements(functional requirements) example, the ability to send messages in near real-time to friends.
	• indirect requirements(non-functional requirements (NFRs)) example, messaging service performance shouldn’t degrade with increasing user load.

2• Handle data
identify and understand data and its characteristics in order to look for "appropriate data storage systems and data processing components" for the system design.
	• What’s the size of the data right now?
	• At what rate is the data expected to grow over time?
	• How will the data be consumed by other subsystems or end users?
	• Is the data read-heavy or write-heavy?
	• Do we need strict consistency of data, or will eventual consistency work?
	• What’s the durability target of the data?
	• What privacy and regulatory requirements do we require for storing or transmitting user data?

3• Discuss the components
figuring out which components we’ll use, where they’ll be placed, and how they’ll interact with each other.
like will a conventional database work, or should we use a NoSQL database?
Front-end components, load balancers, caches, databases, firewalls, and CDNs are just some examples of system components.

4• Discuss trade-offs
	• Different components have different pros and cons. We’ll need to carefully weigh what works for us.
	• Different choices have different costs in terms of money and technical complexity. We need to efficiently utilize our resources.
	• Every design has its weaknesses. As designers, we should be aware of all of them, and we should have a follow-up plan to tackle them.
We should point out weaknesses in our design to our interviewer and explain why we haven’t tackled them yet.

Don't:
===
	• Don’t write code in a system design interview.
	• Don’t start building without a plan.
	• Don’t work in silence.
	• Don’t describe numbers without reason. We have to frame it.
	• If we don’t know something, we don’t paper over it, and we don’t pretend to know it.
Note: If an interviewer asks us to design a system we haven’t heard of, we should just be honest and tell them so. The interviewer will either explain it to us or they might change the question.


Steps to build large-scale distributed systems:
===
1. Determine system requirements and constraints: Initially, identify what the system needs to do (functional requirements) and any limitations it must operate within (non-functional requirements and constraints).

2. Recognize components: Based on the requirements, select the appropriate components, technologies, or APIs that will form part of the system.

3. Generate design: Create a design that integrates the selected components in a way that meets the various requirements.

4. Identify shortcomings in the initial design: Review the initial design to find any limitations or areas for improvement.

5. Discuss trade-offs and improve iteratively: Finally, optimize the design by considering different trade-offs and making iterative improvements.


1. What is a system design interview?
interactive nature, importance of discussing trade-offs, different from coding interviews due to their higher level of abstraction, the process matters more than the final answer, and they often include questions about how a design might evolve over time.
Additionally, the theory of system design comes from the domain of distributed systems.

2. How to prepare for success?
Your approach to preparation, including going through courses, reading technical blogs, understanding how popular applications work and building side projects and participating in mock interviews.
Focusing on trade-offs rather than mechanics is also crucial.

3. How to perform well?
You’ve outlined a good strategy for performing well by engaging the interviewer, refining requirements, designing components, and discussing trade-offs. Remember, it’s important to avoid looking unoriginal and to have a plan to attack the problem.
Identifying and understanding data and its characteristics can help in choosing the right systems and components.
Moreover, acknowledging that there’s no one correct answer to a design problem and every design has its weaknesses is vital.


Section: Introduction:
=====
System design is the process of defining components and their integration, APIs, and data models to build large-scale systems that meet a specified set of functional and non-functional requirements.

• Reliable systems handle faults, failures, and errors.
• Effective systems meet all user needs and business requirements.
• Maintainable systems are flexible and easy to scale up or down. The ability to add new features also comes under the umbrella of maintainability.

16 crucial building blocks

recommend two iterations—first, where we do our best to come up with a design (that takes about 80 percent of our time), and a second iteration for improvements.



Section: Abstractions:
=====
Abstraction is the art of obfuscating details that we don’t need. It allows us to concentrate on the big picture.

Transactions is a database abstraction that hides many problematic outcomes when concurrent users are reading, writing, or mutating the data and gives a simple interface of commit, in case of success, or abort, in case of failure.

Abstractions in distributed systems help engineers simplify their work and relieve them of the burden of dealing with the underlying complexity of the distributed systems.

Network Abstractions: Remote Procedure Calls(RPCs)
RPC method is similar to calling a local procedure, except that the called procedure is usually executed in a different process and on a different computer.


Consistency Models:
===
we can look into the consistency guarantees provided by S3 to decide whether to use it or not. We no need to worry how it handles the consistency, that is abstraction here.

There is a difference between consistency in ACID properties and consistency in the CAP theorem.

• Eventual consistency: ensures that all the replicas converge on a final value after a finite time and when no more writes are coming in.
All nodes gives same value only, but the old one at the time just someone updating.
Ensures high availability.
Eg: DNS, Cassandra(NoSQL database)

• Causal consistency: works by categorizing operations into dependent(causally-related operations) and independent operations.
preserves the order of the causally-related operations.

x=a
b=x+5
y=b

read of x, write of y are causally-related
used in commenting system, like replies to a comment(we want to display comments after the comment it replies to)

• Sequential consistency: preserves the ordering specified by each client’s program.
Eg: social networking applications(don't care post order, but still anticipate single friend’s posts to appear in the correct order in which they were created)

• Strict consistency aka linearizability: ensures that a read request from any replicas will get the latest write value.
Once the client receives the acknowledgment that the write operation has been performed, other clients can read that value.

Linearizability is challenging to achieve in a distributed system. Some of the reasons for such challenges are variable network delays and failures.

Usually, synchronous replication is one of the ingredients for achieving strong consistency, though it in itself is not sufficient.
We might need consensus algorithms such as Paxos and Raft to achieve strong consistency.

Linearizability affects the system’s availability, which is why it’s not always used.
Applications with strong consistency requirements use techniques like quorum-based replication to increase the system’s availability.

* Trade-off: Application programmers have to compromise performance and availability if they use services with strong consistency models.


Failure Models:
===
• Fail-stop: a node halts permanently, other nodes can still detect that node by communicating with it.
• Crash: a node halts silently, other nodes can’t detect that the node has stopped working.
• Omission failures: node fails to send or receive messages
send omission failure - node fails to respond to the incoming request
receive omission failure - node fails to receive the request and thus can’t acknowledge it
• Temporal failures: node generates correct results, but is too late to be useful. This failure could be due to bad algorithms, a bad design strategy, or a loss of synchronization between the processor clocks.
• Byzantine failures: node exhibits random behavior like transmitting arbitrary messages at arbitrary times, producing wrong results, or stopping midway.
This mostly happens due to an attack by a malicious entity or a software bug.
most challenging type of failure to deal with.



Section: Non-functional System Characteristics:
=====
1• Availability - time loss: percentage of time a system accepts requests and responds to clients.
We measure availability as a number of nines.
99.9% (3 nines)		8.76 hours/year		43.8 minutes/month 		10.1 minutes/week downtime

2• Reliability - frequency and impact of failures: probability that the service will perform its functions for a specified time.
how well a system performs its intended operations (functional requirements).
R measured - mean time between failures (MTBF) and mean time to repair (MTTR).
We strive for a higher MTBF value and a lower MTTR value.
we use MTTF instead of MTBF for those cases where a failed component is replaced due to irreparable problems.
A is a function of R. This means that the value of R can change independently, and the value of A depends on R.

3• Scalability: ability of a system to handle an increasing amount of workload without compromising performance.

Workload types:
	• Request workload: 			number of requests served by the system.
	• Data/storage workload: 	amount of data stored by the system.
Dimensions:
	• Size scalability: system can scale simply by adding user or resource.
	• Administrative scalability: system’s ability to manage an increasing number of users or organizations without a significant increase in administrative effort or complexity.
	• Geographical scalability: system can readily service a broad geographical region, as well as a smaller one.
Different approaches of scalability:
	• Vertical scalability—scaling up: prroviding additional capabilities (for example, additional CPUs or RAM) to an existing device.
	allows us to expand our present hardware or software capacity, but we can only grow it to the limitations of our server.
	The dollar cost of vertical scaling is usually high because we might need exotic components to scale up.
	• Horizontal scalability—scaling out: increasing the number of machines in the network.
	We use "commodity nodes"(inexpensive compared to specialized or high-end hardware) for this purpose because of their attractive dollar-cost benefits.
	The catch here is that we need to build a system such that many nodes could collectively work as if we had a single, huge server.

4• Maintainability: keeping the system up and running by finding and fixing bugs, adding new functionalities, keeping the system’s platform updated, and ensuring smooth system operations.
	• Operability: This is the ease with which we can ensure the system’s smooth operational running under normal circumstances and achieve normal conditions under a fault.
	• Lucidity: This refers to the simplicity of the code. The simpler the code base, the easier it is to understand and maintain it, and vice versa.
	• Modifiability: This is the capability of the system to integrate modified, new, and unforeseen features without any hassle.

probability that the service will restore its functions within a specified time of fault occurrence.
M measured - mean time to repair (MTTR)

Maintainability refers to time-to-repair, whereas
Reliability refers to both time-to-repair and the time-to-failure

5• Fault Tolerance: refers to a system’s ability to execute persistently even if one or more of its components fail.
• Replication: We can swap out failed nodes with healthy ones and a failed data store with its replica.

* Trade-off: When a system needs strong consistency, we can synchronously update data in replicas. However, this reduces the availability of the system.
We can also asynchronously update data in replicas when we can tolerate eventual consistency, resulting in stale reads until all replicas converge.
We compromise either on availability or on consistency under failures—a reality that is outlined in the CAP theorem.

• Checkpointing: technique that saves the system’s state in stable storage for later retrieval in case of failures due to errors or service disruptions.
	• Consistent state: all the individual processes of a system have a consistent view of the shared state or sequence of events that have occurred in a system.
	 • All updates to data that were completed before the checkpoint are saved. Any updates to data that were in progress are rolled back as if they didn’t initiate.
	 • Checkpoints include all the messages that have been sent or received up until the checkpoint. No messages are in transit (in-flight) to avoid cases of missing messages.
	 • Relationships and dependencies between system components and their states match what would be expected during normal operation.
	• Inconsistent state: This is a state where there are discrepancies in the saved state of different processes of a system.
	checkpoints across different processes are not coherent and coordinated.



Section: Back-of-the-envelope calculations (BOTECs): involve swift, approximate, and simplified estimations or computations typically done on paper or, figuratively, on the back of an envelope.
=====

Types of data center servers:
===
1• Web servers: first point of contact after load balancers.
mostly serve static content to the client

2• Application servers: run the core application software and business logic.
primarily provide dynamic content

3• Storage servers
mainly include structured (for example, SQL) and nonstructured (NoSQL) data management systems.
YouTube uses the following data stores:
1. Blob storage: This is used for its encoded videos.
2. Temporary processing queue storage: This can hold a few hundred hours of video content uploaded daily to YouTube for processing.
3. Bigtable(NoSQL, key-value store, large amount of structured and semi-structured data, by Google): This is a specialized storage used for storing a large number of thumbnails of videos.
4. Relational database management system (RDBMS): This is for users’ and videos’ metadata (comments, likes, user channels, and so on).
Other data stores are still used for analytics, for example, Hadoop’s HDFS.

Organizations also require servers for services like configuration, monitoring, load balancing, analytics, accounting, caching, and so on.

• Typical Server:
Processor 					Intel Xeon (Sapphire Rapids 8488C)
Number of cores			64 cores
RAM									256 GB
Cache (L3)					112.5 MB
Storage capacity		16 TB

• Important Latencies:
Component														Time (nanoseconds)
L1 cache reference									0.9 
L2 cache reference									2.8 
L3 cache reference									12.9 
Main memory reference								100 
Compress 1KB with Snzip							3,000 (3 microseconds)
Read 1 MB sequentially from memory	9,000 (9 microseconds)
Read 1 MB sequentially from SSD			200,000 (200 microseconds)
Round trip within same datacenter		500,000 (500 microseconds)
Read 1 MB sequentially from SSD with speed ~1GB/sec SSD			1,000,000 (1 milliseconds)
Disk seek														4,000,000 (4 milliseconds)
Read 1 MB sequentially from disk		2,000,000 (2 milliseconds)
Send packet SF->NYC									71,000,000 (71 milliseconds)

Remember the order of magnitude difference between different components and operations is more important than remembering the exact numbers.
For example, we should know that doing IO-bound work (for example, reading 1 MB data sequentially from the SSD disk - 200 micro) is two orders of magnitude slower than CPU-bound work (for example, compressing 1 KB data as snzip - 3 micro).

• Throughput - measured as queries per second (QPS)
Important Rates:
QPS handled by MySQL						1000						needs to go through query planning
QPS handled by key-value store	10,000					simpler API (put and get)
QPS handled by cache server			100,000–1 M			read and write operations

For real projects, initial designs use BOTECs similar to the ones we use in a system design interview.

Transaction Processing Performance Council Benchmark C(TPC-C) is a benchmark to compare the performance of online transaction processing systems.

• Request types:
	• CPU-bound requests: processor - X
	• Memory-bound requests: memory - 10X
	• IO-bound requests: IO subsystem (such as disks or the network) - 100X

BOTECs for making quick, high-level estimates and decisions in the early stages.

• Resource Estimations:
	• Servers needed = No.of request/s / request a server can handle
		Peak capacity
			• Improving the RPS of a server
			• Improving over the peak load assumption
		Cost of server = $3.54/h ($2.6/h with 1y term saving plan)
	• Storage requirements
	• Bandwidth requirements = ((daily incoming & outgoing data)Bytes / 86400) * 8bits Gbps(Gigabits per second)

Plausibility test: evaluate assumptions, estimates, or outcomes seem reasonable and realistic

Poisson distribution: gives probability of an outcome, number of events occurring with in a time

Pareto principle: assumption that 80% of our peak traffic occurs within 20% of the time

In 2023, 20 TB disks are readily available.
Many data centers of an organization are connected with high speed networks such as 1Tbps.




Structure of Notes:
===
2. Building Blocks:
Introduction
DNS
Load balancers
Databases
Key-value store
CDN
Sequencer
Distributed monitoring
Monitoring server-side errors
Monitoring client-side errors
Distributed cache
Distributed messaging queue
Pub-sub system
Rate limiter
Blob store
Distributed search
Distributed logging
Distributed task scheduler
Sharded counters
Concluding building blocks



Section: Building Blocks: Introduction:
=====
Many of the building blocks we discuss are also available for actual use in the public clouds, such as Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP).

Section: 1• Domain Name System(DNS):
=====
Internet’s naming service that maps human-friendly domain names to machine-readable IP addresses.
mappings of domain names to IP addresses.

When a user enters a domain name in the browser, the browser has to translate the domain name to IP address by asking the DNS infrastructure.

Name servers: DNS servers(among many servers) that respond to users’ queries.
Resource records(RR): form of domain name to IP address mappings.
	Common Types:
	A				Provides the hostname to IP address mapping
	NS			Provides the hostname that is the authoritative DNS for a domain name
	CNAME		Provides the mapping from alias to canonical hostname
	MX			Provides the mapping of mail server from alias to canonical hostname
Caching
Hierarchy: allows DNS to be highly scalable

DNS hierarchy:
Types:
• DNS resolver / local or default servers:
	initiate the querying sequence and forward requests to the other DNS name servers
	can also cater to users’ DNS queries through caching techniques
• Root-level name servers:
	receive requests from local servers
	return a list of top-level domain (TLD) servers that hold the IP addresses of the .xyz domain
• Top-level domain (TLD) name servers: hold the IP addresses of authoritative name servers.
• Authoritative name servers: organization’s DNS name servers that provide the IP addresses of the web or application servers

Ways:
• Iterative: The local server requests the root, TLD, and the authoritative servers for the IP address.
• Recursive: The end user requests the local server. The local server further requests the root DNS name servers. The root name servers forward the requests to other name servers.

iterative query is preferred to reduce query load on DNS infrastructure.

third-party public DNS resolvers offered by Google, Cloudflare, OpenDNS, and many more may provide quicker responses than the local ISP DNS facilities.

Caching: temporary storage of frequently requested resource records.
can be implemented in the browser, operating systems, local name server within the user’s network, or the ISP’s DNS resolvers (Total in 4 locations).

DNS as a distributed system:
===
advantages:
• It avoids becoming a single point of failure (SPOF).
• It achieves low query latency so users can get responses from nearby servers.
• It gets a higher degree of flexibility during maintenance and updates or upgrades. For example, if one DNS server is down or overburdened, another DNS server can respond to user queries.

There are 13 logical root name servers (named letter A through M) with many instances spread throughout the globe. These servers are managed by 12 different organizations.

Highly scalable:
Roughly 1,000 replicated instances of 13 root-level servers are spread throughout the world strategically to handle user queries.
different services handle different portions of the tree enabling scalability and manageability of the system

Reliable:
Caching:
Server replication: has replicated copies of each logical server spread systematically across the globe
Protocol: many rely on unreliable User Datagram Protocol (UDP) which is much faster to request and receive DNS responses
UDP is usually favored over TCP(which needs a three-way handshake every time before data exchange)

can use TCP when its message size exceeds the original packet size of 512 Bytes.
Some clients prefer TCP over UDP to employ transport layer security for privacy reasons.

Consistent:
strong consistency to achieve high performance because data is read frequently from DNS databases as compared to writing.
eventual consistency and updates records on replicated servers lazily.
few seconds up to three days to update records on the DNS servers across the Internet.

possible that certain resource records are updated on the authoritative servers in case of server failures at the organization.
To mitigate this issue, each cached record comes with an expiration time called time-to-live (TTL).

maintain high availability - TTL low - as low as 120 seconds

Test it out:
During nslookup "Non-authoritative answer" (server that is not the authoritative server of Google) means Cached response from others
order different some times due to DNS indirectly performing laod-balancing

dig command gives TTL and Query time(DNS response time)

OS have configuration files (/etc/resolv.conf in Linux) with the DNS resolvers’ IP addresses
Often, DHCP provides the default DNS resolver IP address along with other configurations.
Typically, the Berkeley Internet Name Domain (BIND) software is used on DNS resolvers.
The InterNIC maintains the updated list of 13 root servers.



Section: 2• Load Balancers:
=====
fairly divide all clients’ requests among the pool of available servers.
avoid overloading or crashing servers
first point of contact within a data center after the firewall
many not be required if few thousands of requests per second

Capabilities:
• Scalability: upscaling or downscaling of capacity of the application/service transparent to the end users
• Availability: hide faults and failures of server
• Performance: forward requests to servers with a lesser load, not only improves performance but also improves resource utilization

Placing load balancers:
Generally sit between clients and servers
• between user and web servers/application gateway
• between web servers and application servers that run the business/application logic
• between application servers and database servers
In reality, load balancers can be potentially used between any two services with multiple instances within the design of a system.

Services offered:
• Health checking: heartbeat protocol to monitor the health and, therefore, reliability of end-servers. 
• TLS termination(TLS/SSL offloading): reduce the burden on end-servers by handling TLS termination with the client.
• Predictive analytics: predict traffic patterns through analytics performed over traffic passing through them or using statistics of traffic obtained over time.
• Reduced human intervention: automation of handling failures.
• Service discovery: clients’ requests are forwarded to appropriate hosting servers by inquiring about the service registry.
• Security: mitigating attacks like denial-of-service (DoS) at different layers of the OSI model (layers 3, 4, and 7).

As a whole, load balancers provide flexibility, reliability, redundancy, and efficiency to the overall design of the system.

If one load balancer fails, and there’s nothing to failover to, the overall service will go down.
Generally, to maintain high availability, enterprises use clusters of load balancers that use heartbeat communication to check the health of load balancers at all times.

• Global server load balancing (GSLB): distribution of traffic load across multiple geographical regions.
• Local load balancing: achieved within a data center. focuses on improving efficiency and better resource utilization of the hosting servers in a data center.

globally arriving traffic load is intelligently forwarded to a data center
decisions based on the users’ geographic locations, the number of hosting servers in different locations, the health of data centers, and so on.

can be installed on-premises or obtained through Load Balancing as a Service (LBaaS) (cloud-based service, provides flexibility of automatic scaling up or down depending upon the traffic load)

Load balancing in DNS: uses round-robin to reorders the list of IP addresses in response to each DNS query, so load distrubted among data centers
Limitations:
	• Different ISPs have a different number of users. An ISP serving many customers will provide the same cached IP to its clients, resulting in uneven load distribution on end-servers.
	• Because the round-robin load-balancing algorithm doesn’t consider any end-server crashes, it keeps on distributing the IP address of the crashed servers until the TTL of the cached entries expires. Availability of the service, in that case, can take a hit due to DNS-level load balancing.

DNS isn’t the only form of GSLB. Application delivery controllers (ADCs) and cloud-based load balancing are better ways to do GSLB.

Application delivery controllers (ADCs) are part of the application delivery network (ADN).
They can be considered the superset of LBs offering various services, including load balancing.
The primary task of ADCs is to perform web acceleration to reduce the load from the server farm.
Some well-known services between layers 3 and 7 include caching, SSL offloading, proxy/reverse proxy services, IP traffic optimization, and many more.
ADCs also implement GSLB.

Need for local load balancers:
DNS limitations:
	• The small size of the DNS packet (512 Bytes) isn’t enough to include all possible IP addresses of the servers.
	• There’s limited control over the client’s behavior. Clients may select arbitrarily from the received set of IP addresses. Some of the received IP addresses may belong to busy data centers.
	• Clients can’t determine the closest address to establish a connection with.
	• In case of failures, recovery can be slow through DNS because of the caching mechanism, especially when TTL values are longer.

Local load balancing: reside within a data center, behave like a reverse proxy and balance load
Incoming clients’ requests seamlessly connect to the LB that uses a virtual IP address (VIP - no physical machine, group of machines will use the same address)
DNS queries are usually responded with VIPs.

Global traffic management (GTM):
	• GTM through ADCs: Some ADCs implement GSLB. In that case, ADCs have a real-time view of the hosting servers and forward requests based on the health and capacity of the data center.
	• GTM through DNS: DNS does GSLB by analyzing the IP location of the client. For each user requesting IP for a domain name (for example, www.educative.io), DNS-based GSLB forwards the IP address of the data center geographically closer to the requesting IP location.

Algorithms:
	• Round-robin scheduling: In this algorithm, each request is forwarded to a server in the pool in a repeating sequential manner.
	• Weighted round-robin: If some servers have a higher capability of serving clients’ requests, then it’s preferred to use a weighted round-robin algorithm. In a weighted round-robin algorithm, each node is assigned a weight. LBs forward clients’ requests according to the weight of the node. The higher the weight, the higher the number of assignments.
	• Least connections: In certain cases, even if all the servers have the same capacity to serve clients, uneven load on certain servers is still a possibility. For example, some clients may have a request that requires longer to serve. Or some clients may have subsequent requests on the same connection. In that case, we can use algorithms like least connections where newer arriving requests are assigned to servers with fewer existing connections. LBs keep a state of the number and mapping of existing connections in such a scenario. We’ll discuss more about state maintenance later in the lesson.
	• Least response time: In performance-sensitive services, algorithms such as least response time are required. This algorithm ensures that the server with the least response time is requested to serve the clients.
	• IP hash: Some applications provide a different level of service to users based on their IP addresses. In that case, hashing the IP address is performed to assign users’ requests to servers.
	• URL hash: It may be possible that some services within the application are provided by specific servers only. In that case, a client requesting service from a URL is assigned to a certain cluster or set of servers. The URL hashing algorithm is used in those scenarios.

	other algorithms also, like randomized or weighted least connections algorithms

Algorithm types:
	• Static algorithms don’t consider the changing state of the servers. Therefore, task assignment is carried out based on existing knowledge about the server’s configuration. Naturally, these algorithms aren’t complex, and they get implemented in a single router or commodity machine where all the requests arrive.
	• Dynamic algorithms are algorithms that consider the current or recent state of the servers. Dynamic algorithms maintain state by communicating with the server, which adds communication overhead. State maintenance makes the design of the algorithm much more complicated.

In practice, dynamic algorithms provide far better results because they maintain a state of serving hosts and are, therefore, worth the effort and complexity.

Session maintenance through LBs:
	• Stateful load balancing: maintaining a state of the sessions established between clients and hosting servers.
	Uses state in algorithm
	Essentially, the stateful LBs retain a data structure that maps incoming clients to hosting servers. Stateful LBs increase complexity and limit scalability because session information of all the clients is maintained across all the load balancers.
	• Stateless load balancing: maintains no state and is, therefore, faster and lightweight.
	Use consistent hashing to make forwarding decisions.
	However, "if infrastructure changes" (for example, a new application server joining), stateless LBs may not be as resilient as stateful LBs because consistent hashing alone isn’t enough to route a request to the correct application server. Therefore, a local state may still be required along with consistent hashing.

Types:
	• Layer 4 load balancers(stateless?): Layer 4 refers to the load balancing performed on the basis of transport protocols like TCP and UDP. These types of LBs maintain connection/session with the clients and ensure that the same (TCP/UDP) communication ends up being forwarded to the same back-end server. Even though TLS termination is performed at layer 7 LBs, some layer 4 LBs also support it.
	• Layer 7 load balancers(stateful?): Layer 7 load balancers are based on the data of application layer protocols. It’s possible to make application-aware forwarding decisions based on HTTP headers, URLs, cookies, and other application-specific data—for example, user ID. Apart from performing TLS termination, these LBs can take responsibilities like rate limiting users, HTTP routing, and header rewriting.

Layer 7 load balancers are smart in terms of inspection.
However layer 4 load balancers are faster in terms of processing.
	Google has developed a high-performance distributed load balancer called Maglev for their cloud platform.

ALB, NLB

Deployment:
	A traditional data center may have a three-tier LB
	• Tier-0 and Tier-1 LBs: If DNS can be considered as the tier-0 load balancer, Equal-cost Multi-path (ECMP) routers are the tier-1 LBs.
	ECMP routers, equal routing priority, play a vital role in the horizontal scalability of the higher-tier LBs.
	• Tier-2 LBs: include layer 4 load balancers
	• Tier-3 LBs: Layer 7 LBs, maintain state, perform health monitoring of servers at HTTP level, reduces the burden on end-servers by handling low-level details like TCP-congestion control protocols, the discovery of Path MTU (maximum transmission unit), the difference in application protocol between client and back-end servers, and so on.

tier 1 balances the load among the load balancers themselves.
Tier 2 enables a smooth transition from tier 1 to tier 3 in case of failures, whereas 
tier 3 does the actual load balancing between back-end servers.
Each tier performs other tasks to reduce the burden on end-servers.

HAProxy is an open-source software LB that can perform load balancing based on TCP and HTTP.

Direct Routing (DR) or Direct Server Return (DSR), which is a method where the back-end server bypasses certain load balancers in the response path.
DR/DSR is typically used for non-HTTPS traffic because when TLS termination(HTTPs) is involved, responses need to be encrypted before reaching the client, requiring them to pass back through the Layer 7 load balancer for "encryption".

Implementation:
	• Hardware load balancers: work as stand-alone devices and are quite expensive
	• Software load balancers
		flexibility, programmability, and cost-effectiveness.
		implemented on commodity hardware.
		provide predictive analysis that can help prepare for future traffic patterns
	• Load Balancers as a Service (LBaaS)
		may not necessarily replace a local on-premise load balancing facility, but they can perform global traffic management between different zones
		ease of use, management, metered cost, flexibility in terms of usage, auditing, and monitoring services to improve business decisions

Client-side load balancing is suited where there are numerous services, each with many instances (such as load balancing in Twitter)




Section: 3• Databases:
=====
Limitations of file storage
	• We can’t offer concurrent management to separate users accessing the storage files from different locations.
	• We can’t grant different access rights to different users.
	• How will the system scale and be available when adding thousands of entries?
	• How will we search content for different users in a short time?

Database is an organized collection of data that can be managed and accessed easily.
created to make it easier to store, retrieve, modify, and delete data in connection with different data-processing procedures.

World Data Center for Climate (WDCC) is the largest database in the world, containing around 220 TB of web data and 6 PB of additional data.

Advantages:
	• Managing large data: A large amount of data can be easily handled with a database, which wouldn’t be possible using other tools.
	• Retrieving accurate data (data consistency): Due to different constraints in databases, we can retrieve accurate data whenever we want.
	• Easy updation: It is quite easy to update data in databases using data manipulation language (DML).
	• Security: Databases ensure the security of the data. A database only allows authorized users to access data.
	• Data integrity: Databases ensure data integrity by using different constraints for data.
	• Availability: Databases can be replicated (using data replication) on different servers, which can be concurrently updated. These replicas ensure availability.
	• Scalability: Databases are divided (using data partitioning) to manage the load on a single node. This increases scalability.

Types:
===
1• SQL (relational databases):
	has a well defined structure such as attributes (columns of the table).
	adhere to particular schemas before storing the data.
	unique key
	foreign keys
	Structure Query Language (SQL)
	simplicity, robustness, flexibility, performance, scalability, and compatibility in managing generic data.
	* Trade-off: Traditional databases are vertically scalable. Vertical scaling has limits. One might reach a point when more compute, memory, storage, or networking capability could not be added to a single node.
	provide the atomicity, consistency, isolation, and durability (ACID) properties to maintain the integrity of the database
	for structured data storage
	ACID hides many anomalies (like dirty reads, dirty writes, read skew, lost updates, write skew, and phantom reads) behind a simple transaction abort.
*ACID:
	• Atomicity: A "transaction is considered an atomic unit". Therefore, either all the statements within a transaction will successfully execute, or none of them will execute. If a statement fails within a transaction, it should be aborted and rolled back.
	• Consistency: At any given time, the database should be in a consistent state, and it should remain in a consistent state after every transaction. For example, if multiple users want to view a record from the database, it should return a similar result each time.
	• Isolation: In the case of multiple transactions running concurrently, they shouldn’t be affected by each other. The final state of the database should be the same as the transactions that were executed sequentially.
	• Durability: The system should guarantee that completed transactions will survive permanently in the database even in system failure events.
Eg: MySQL,Oracle Database, Microsoft SQL Server, IBM DB2, Postgres, SQLite
Features:
	• Flexibility:
		Data definition language (DDL) provides us the flexibility to modify the database, including tables, columns, renaming the tables, and other changes.
		DDL even allows us to modify schema while other queries are happening and the database server is running.
	• Reduced redundancy:
		eliminates data redundancy. The information related to a specific entity appears in one table while the relevant data to that specific entity appears in the other tables linked through foreign keys. This process is called normalization and has the additional benefit of removing an inconsistent dependency.
	• Concurrency:
		handled through transactional access to the data
		transaction is considered an atomic operation, so it also works in error handling to either roll back or commit a transaction on successful execution
	• Integration:
		The process of aggregating data from multiple sources is a common practice in enterprise applications. A common way to perform this aggregation is to integrate a shared database where multiple applications store their data. This way, all the applications can easily access each other’s data while the concurrency control measures handle the access of multiple applications.
	• Backup and disaster recovery:
		export and import operations make backup and restoration easier.
		cloud-based relational databases perform "continuous mirroring" to avoid loss of data and make the restoration process easier and quicker.
Drawback:
	• Impedance mismatch:
		difference between the relational model and the in-memory data structures
		the values in a table take simple values that can’t be a structure or a list
		for in-memory, where a complex data structure can be stored
		To make the complex structures compatible with the relations, we would need a translation of the data in light of relational algebra.

2• NoSQL (non-relational databases):
	such as document databases often have application-defined structure of data.
	dynamic schema.
	used in applications that require a large volume of semi-structured and unstructured data, low latency, and flexible data models
Characteristics:
	• Simple design:
		doesn’t require dealing with the impedance mismatch
	• Horizontal scaling:
		ability to run databases on a large cluster
		stored in one document instead of multiple tables over nodes
		often spread data across multiple nodes and balance data and queries across nodes automatically
	• Availability:
		transparently replace without any application disruption, during node failure
		support data replication
	• Support for unstructured and semi-structured data:
		work with data that doesn’t have schema at the time of database configuration or data writes
		For example, document databases are structureless; they allow documents (JSON, XML, BSON, and so on) to have different fields.
		For example, one JSON document can have fewer fields than the other.
	•	Cost:
		open source and freely available, but RDBMSs expensive.
		use clusters of cheap commodity servers, but RDBMSs costly proprietary hardware and storage systems.
Types:
	• Key-value database:
		use key-value methods like hash tables to store data in key-value pairs
		key serves as a unique or primary key
		Eg: Amazon DynamoDB, Redis, and Memcached DB

		Efficient for "session-oriented applications", such as web applications, store users’ data in the main memory or in a database during a session.
		This data may include user profile information, recommendations, targeted promotions, discounts, and more.
		A unique ID (a key) is assigned to each user’s session for easy access and storage.
	• Document database:
		designed to store and retrieve documents in formats like XML, JSON, BSON, and so on.
		documents are composed of a hierarchical tree data structure that can include maps, collections, and scalar values.
		Documents in this type of database may have varying structures and data.
		Eg: MongoDB and Google Cloud Firestore

		For example, in "e-commerce applications", a product has thousands of attributes, which is unfeasible to store in a relational database due to its impact on the reading performance. Here comes the role of a document database, which can efficiently store each attribute in a single file for easy management and faster reading speed.

		good option for "content management applications", such as blogs and video platforms.

		An entity required for the application is stored as a single document in such applications.
	• Graph database:
		use the graph data structure to store data, where nodes represent entities, and edges show relationships between entities.
		The organization of nodes based on relationships leads to interesting patterns between the nodes.
		This database allows us to store the data once and then interpret it differently based on relationships.
		Graph data is kept in store files for persistent storage. Each of the files contains data for a specific part of the graph, such as nodes, links, properties, and so on.
		Eg: Neo4J, OrientDB, and InfiniteGraph.

		can be used in "social applications" and provide interesting facts and figures among different kinds of users and their activities.
		The focus of graph databases is to store data and pave the way to drive analyses and decisions based on relationships between entities.
		The nature of graph databases makes them suitable for various applications, such as "data regulation and privacy, machine learning research, financial services-based applications", and many more.
	• Columnar database:
		store data in columns instead of rows.
		enable access to all entries in the database column quickly and efficiently.
		Eg: HBase, Hypertable, and Amazon Redshift.

		efficient for a large number of aggregation and data analytics queries.
		It drastically reduces the disk I/O requirements and the amount of data required to load from the disk.

		For example, in applications related to "financial institutions", there’s a need to sum the financial transaction over a period of time. Columnar databases make this operation quicker by just reading the column for the amount of money, ignoring other attributes of customers.
Drawbacks:
	• Lack of standardization:
		doesn’t follow any specific standard, like how relational databases follow relational algebra.
		Porting applications from one type of NoSQL database to another might be a challenge.
	• Consistency:
		* Trade-off: between consistency and availability when failures can happen.
		We won’t have strong data integrity, like primary and referential integrities in a relational database.
		Data might not be strongly consistent but slowly converging using a weak model like eventual consistency.

Google’s Cloud Spanner is one such database that’s geo-replicated with automatic horizontal sharding ability and high-speed global snapshots of data.


Data Replication:
=====
keeping multiple copies of the data at various nodes (preferably geographically distributed) to achieve availability, scalability, and performance.

Complexities:
	• How do we keep multiple copies of data consistent with each other?
	• How do we deal with failed replica nodes?
	• Should we replicate synchronously or asynchronously?
		• How do we deal with replication lag in case of asynchronous replication?
	• How do we handle concurrent writes?
	• What consistency model needs to be exposed to the end programmers?

Ways:
	• Synchronous replication: primary node waits for acknowledgments from secondary nodes
		After receiving acknowledgment from all secondary nodes, the primary node reports success to the client.
		If one of the secondary nodes doesn’t acknowledge due to failure or fault in the network, the primary node would be unable to acknowledge the client until it receives the successful acknowledgment from the crashed node. This causes high latency in the response from the primary node to the client.
	• Asynchronous replication: primary node doesn’t wait for the acknowledgment from the secondary nodes and reports success to the client after updating itself.
		primary node can continue its work even if all the secondary nodes are down. However, if the primary node fails, the writes that weren’t copied to the secondary nodes will be lost.

* Trade-off: between data consistency and availability when different components of the system can fail.

Models:
1• Single leader/primary-secondary replication
===
One primary, responsible for processing any writes to data stored on the cluster, sends all the writes to the secondary nodes and keeps them in sync.
when our workload is read-heavy (inappropriate if our workload is write-heavy)
read resilient
reads can be done from primary or secondary nodes

inconsistency if we use asynchronous replication
if the primary node fails, any missed updates not passed on to the secondary nodes can be lost

In case of failure of the primary node, a secondary node can be appointed as a primary node, which speeds up the process of recovering the initial primary node by Manual(decide primary and notifies all secondary nodes) or Automatic(primary which wins leader election(based on factor, such as processor with the highest identifier)).

the primary node is a bottleneck and a single point of failure. Moreover, it helps to "achieve read scalability" but "fails to provide write scalability"

Methods:
	• Statement-based replication (SBR)
		used in MySQL databases
		after executing statements, they are written in log file and sent to Secondary for execution.
		used in MySQL before version 5.1.
		some disadvantages, like nondeterministic(UPDATE and DELETE statements that use a LIMIT clause without an ORDER BY clause are considered nondeterministic) functions such as NOW() might result in distinct writes on the primary and secondary nodes.
	• Write-ahead log (WAL) shipping
		used in both PostgreSQL and Oracle
		maintains transactional logs(in disk) instead of SQL statements into a log file
		Subsequently, the recorded operations are executed on the primary database before being transmitted to secondary nodes for execution
		Drawback - its tight coupling with the inner structure of the database engine, making software upgrades on the leader and followers complicated.
	• Logical (row-based) replication
		in various relational databases, including PostgreSQL and MySQL
		changes made to the database are captured at the level of individual rows and then replicated to the secondary nodes
		advantages in terms of flexibility and compatibility with different types of schemas.

2• Multi-leader replication
===
alternative to single leader replication
multiple primary nodes that process the writes and send them to all other primary and secondary nodes to replicate
used in databases along with external tools like the Tungsten Replicator for MySQL.

useful in applications in which we can continue work even if we’re offline
example, calendar application in which we can set our meetings even if we don’t have access to the internet. Once we’re online, it replicates its changes from our local database (our mobile phone or laptop acts as a primary node) to other nodes.

Conflict: Since all the primary nodes concurrently deal with the write requests, they may modify the same data, which can create a conflict between them

Handle Conflicts:
	• Conflict avoidance
		can be avoided if the application can verify that all writes for a given record go via the same leader.
		However, the conflict may still occur if a user moves to a different location and is now near a different data center. If that happens, we need to reroute the traffic. In such scenarios, the conflict avoidance approach fails and results in concurrent writes.
	• Last-write-wins
		update with the latest timestamp is selected.
		create difficulty because the clock synchronization across nodes is challenging in distributed systems. There’s clock skew that can result in data loss.
	• Custom logic
		on conflict, it calls our custom conflict handler(our own logic to handle conflicts according to the needs of our application)

Multi-leader replication topologies:
	many such as circular topology, star topology, and all-to-all topology(most common/used).
	In star and circular topology, there’s again a similar drawback that if one of the nodes fails, it can affect the whole system.

3• Peer-to-peer or leaderless replication
===
resolves the problem of primary-secondary replication by not having single primary node.
All the nodes have equal weightage and can accept read and write requests.
can be found in the Cassandra database.

Like primary-secondary replication, this replication can also yield inconsistency. This is because when several nodes accept write requests, it may lead to concurrent writes. A helpful approach used for solving write-write inconsistency is called quorums.

Quorums:
If we have n nodes, then every write must be updated in at least w nodes to be considered a success, and we must read from r nodes. We’ll get an updated value from reading as long as w+r>n because at least one of the nodes must have an updated write from which we can read. Quorum reads and writes adhere to these r and w values. These n, w, and r are configurable in Dynamo-style databases.
(explained in detail in Key-value store)


Data Partitioning (or sharding):
=====
Traditional databases are attractive due to their properties such as range queries, secondary indices, and transactions with the ACID properties.

enables us to use multiple nodes where each node manages some part of the whole data.
To handle increasing query rates and data amounts, we strive for balanced partitions and balanced read/write load.

hotspots - highly congested partitions

Ways:
===
• Vertical sharding
We can put different tables in various database instances, which might be running on a different physical server.
We might break a table into multiple tables so that some columns are in one table while the rest are in the other.
We should be careful if there are joins between multiple tables. We may like to keep such tables together on one shard.

used to increase the speed of data retrieval from a table consisting of columns with very wide text or a binary large object (blob)
column with large text or a blob is split into a different table.

Vertical sharding has its intricacies and is more amenable to manual partitioning, where stakeholders carefully decide how to partition data.
In comparison, horizontal sharding is suitable to automate even under dynamic conditions.

Creating shards by moving specific tables of a database around is also a form of vertical sharding.

• Horizontal sharding
===
used to divide a table into multiple tables by splitting data row-wise
Each partition of the original table distributed over database servers is called a "shard".

Strategies:
===
• Key-range based sharding: each partition is assigned a continuous range of keys.
Sometimes, a database consists of multiple tables bound by foreign key relationships. In such a case, the horizontal partition is performed using the same partition key on all tables in a relation.

* Trade-off between an impact on increased storage and locating the desired shards efficiently.

Primary keys are unique across all database shards to avoid key collision during data migration among shards and the merging of data in the online analytical processing (OLAP) environment.

Pros:
	easy to implement. We precisely know where (which node, which shard) to look for a specific range of keys.
	Range queries can be performed using the partitioning keys, and those can be kept in partitions in sorted order. How exactly such a sorting happens over time as new data comes in is implementation specific.
Cons:
	can’t be performed using keys other than the partitioning key.
	If keys aren’t selected properly, some nodes may have to store more data due to an uneven distribution of the traffic.

• Hash-based sharding
uses a hash function on an attribute
use a hash function on the key to get a hash value and then mod by the number of partitions

Pros:
	Keys are uniformly distributed across the nodes.
Cons:
	We can’t perform range queries with this technique. Keys will be spread over all partitions.

Number of shards the database should be distributed in = Database size / Size of a single shard

Consistent hashing: assigns each server or item in a distributed hash table a place on an abstract circle, called a ring, irrespective of the number of servers in the table. This permits servers and objects to scale without compromising the system’s overall performance.
(explained in detail in Key-value Store section)

Pros:
	It’s easy to scale horizontally.
	It increases the throughput and improves the latency of the application.
Cons:
	Randomly assigning nodes in the ring may cause non-uniform distribution.

Rebalance the partitions:
===
Query load can be imbalanced:
	• The distribution of the data isn’t equal.
	• There’s too much load on a single partition.
	• There’s an increase in the query traffic, and we need to add more nodes to keep up.

Strategies:
	• Avoid hash mod n:
		problem with the addition or removal of nodes in the case of hashmodn is that every node’s partition number changes and a lot of data moves.
		This moving of keys from one node to another makes rebalancing costly.
	• Fixed number of partitions
		create a, fixed, higher number of partitions than the nodes and assign these partitions to nodes
		used in Elasticsearch, Riak, and many more.
	• Dynamic partitioning
		when the size of a partition reaches the threshold, it’s split equally into two partitions each on one node.
		The number of partitions adapts to the overall data amount
		downside - Dynamic rebalancing during reads and writes is challenging because it involves moving data between nodes, causing latency and potential conflicts.
		used in HBase and MongoDB
	• Partition proportionally to nodes
		number of partitions is proportionate to the number of nodes, which means every node has fixed partitions.
		In earlier approaches, the number of partitions was dependent on the size of the dataset. That isn’t the case here. While the number of nodes remains constant, the size of each partition rises according to the dataset size. However, as the number of nodes increases, the partitions shrink. When a new node enters the network, it splits a certain number of current partitions at random, then takes one half of the split and leaves the other half alone. This can result in an unfair split.
		used by Cassandra and Ketama.

Partitioning and secondary indexes:
Partition secondary indexes by document:
	Each partition is fully independent in this indexing approach. Each partition has its secondary indexes covering just the documents in that partition. It’s unconcerned with the data held in other partitions. If we want to write anything to our database, we need to handle that partition only containing the document ID we’re writing. It’s also known as the local index.
	In the illustration below, there are three partitions, each having its own identity and data. If we want to get all the customer IDs with the name John, we have to request from all partitions.
	However, this type of querying on secondary indexes can be expensive. As a result of being restricted by the latency of a poor-performing partition, read query latencies may increase.
Partition secondary indexes by the term:
	Instead of creating a secondary index for each partition (a local index), we can make a global index for secondary terms that encompasses data from all partitions.
	Partitioning secondary indexes by the term is more read-efficient than partitioning secondary indexes by the document. This is because it only accesses the partition that contains the term. However, a single write in this approach affects multiple partitions, making the method write-intensive and complex.

Request routing:
	• Allow the clients to request any node in the network. If that node doesn’t contain the requested data, it forwards that request to the node that does contain the related data.
	• The second approach contains a routing tier. All the requests are first forwarded to the routing tier, and it determines which node to connect to fulfill the request.
	• The clients already have the information related to partitioning and which partition is connected to which node. So, they can directly contact the node that contains the data they need.

ZooKeeper:
keeps track of all the mappings in the network, and each node connects to ZooKeeper for the information.
used by HBase, Kafka and SolrCloud


Trade-offs:
=====
Both horizontal and vertical sharding involve adding resources to our computing infrastructure. Our business stakeholders must decide which is suitable for our organization. We must scale our resources accordingly for our organization and business to grow, to prevent downtime, and to reduce latency. We can scale these resources through a combination of adjustments to CPU, physical memory requirements, hard disk adjustments, and network bandwidth.

Advantages and disadvantages of a centralized database:
Advantages:
	• Data maintenance, such as updating and taking backups of a centralized database, is easy.
	• Centralized databases provide stronger consistency and ACID transactions than distributed databases.
	• Centralized databases provide a much simpler programming model for the end programmers as compared to distributed databases.
	• It’s more efficient for businesses that have a small amount of data to store that can reside on a single node.
Disadvantages:
	• A centralized database can slow down, causing high latency for end users, when the number of queries per second accessing the centralized database is approaching single-node limits.
	• A centralized database has a single point of failure. Because of this, its probability of not being accessible is much higher.

Advantages and disadvantages of a distributed database (having sharding):
Advantages:
	• It’s fast and easy to access data in a distributed database because data is retrieved from the nearest database shard or the one frequently used.
	• Data with different levels of distribution transparency can be stored in separate places.
	• Intensive transactions consisting of queries can be divided into multiple optimized subqueries, which can be processed in a parallel fashion.
Disadvantages:
	• Sometimes, data is required from multiple sites(locations), which takes more time than expected.
	• Relations are partitioned vertically or horizontally among different nodes. Therefore, operations such as joins need to reconstruct complete relations by carefully fetching data. These operations can become much more expensive and complex.
	• It’s difficult to maintain consistency of data across sites in the distributed database, and it requires extra measures.
	• Updations and backups in distributed databases take time to synchronize data.

Query optimization and processing speed in a distributed database:

a = Total access delay
b = Data rate
v = Total data volume

Total communication time T = a + v/b

<Example with assumptions>



Section: 4• Key-value Store:
=====
are distributed hash tables ("DHTs", a decentralized storage system that provides lookup and storage schemes similar to a hash table, storing key-value pairs.).
key is generated by the hash function and should be unique.
key binds to a specific value and doesn’t assume anything about the structure of the value. A value can be a blob, image, server name, or anything the user wants to store against a unique key.

preferred to keep the size of value relatively smaller (KB to MB). We can put large data in the blob store and put links to that data in the value field.
useful in many situations, such as "storing user sessions" in a web application and "building NoSQL databases".

Many real-world services like Amazon, Facebook, Instagram, Netflix, and many more use primary-key access to a data store instead of traditional online transaction processing ("OLTP", involves gathering input information, processing the data, and updating existing data to reflect the collected and processed information. The OLTP databases carry out day-to-day tasks like insertion, updating, and deletion of the data in the database.) databases.

Eg: bestseller lists, shopping carts, customer preferences, session management, sales rank, and product catalogs.

to overcome the problems of traditional databases.

Design of a Key-value Store:
Functional requirements:
	expected to offer functions such as get and put.
	• Configurable service: Some applications might have a tendency to trade strong consistency for higher availability. We need to "provide a configurable service so that different applications could use a range of consistency models". We need tight "control over the trade-offs between availability, consistency, cost-effectiveness, and performance".
	Note: Such configurations can only be performed when instantiating a new key-value store instance and cannot be changed dynamically when the system is operational.
	• Ability to always write (when we picked “A” over “C” in the context of CAP): The applications should "always have the ability to write into the key-value storage". If the user wants strong consistency, this requirement might not always be fulfilled due to the implications of the CAP theorem.
	• Hardware heterogeneity: We want to add new servers with different and higher capacities, seamlessly, to our cluster without changing or upgrading existing servers. Our system should be "able to accommodate and leverage different capacity servers, ensuring correct core functionality (get and put data) while balancing the workload distribution according to each server’s capacity". This calls for a peer-to-peer design with no distinguished nodes.
Non-functional requirements:
	• Scalability: should run on tens of thousands of servers distributed across the globe.
	Incremental scalability is highly desirable.
	We should add or remove the servers as needed with minimal to no disruption to the service availability.
	Moreover, our system should be able to handle an enormous number of users of the key-value store.
	• Fault tolerance: should operate uninterrupted despite failures in servers or their components.

differ from traditional relational databases:
	simple retrieval by key, 
	lack complex query languages, and often 
	prioritize availability and scalability over strict consistency
	handle unstructured data
advantageous:
	for managing massive amounts of data, 
	enabling high-speed data lookups, and 
	in scenarios where the data structure is flexible or not fully known in advance.

Assumptions:
	• The data centers hosting the service are trusted (non-hostile).
	• All the required authentication and authorization are already completed.
	• User requests and responses are relayed over HTTPS.

API design:
	• The get function:
	get(key)
	return the associated value on the basis of the parameter key.
	When data is replicated, it locates the object replica associated with a specific key that’s hidden from the end user.
	in eventual consistency, there might be more than one value returned against a key.

	• The put function:
	put(key, value)
	stores the value associated with the key.
	system automatically determines where data should be placed.
	Additionally, the system often keeps metadata about the stored object. Such metadata can include the version of the object.

	We often keep hashes of the value (and at times, value + associated key) as metadata for data integrity checks.

Data type:
	The key is often a primary key in a key-value store, while the value can be any arbitrary binary data.

Note: Dynamo uses MD5 hashes on the key to generate a 128-bit identifier. These identifiers help the system determine which server node will be responsible for this specific key.


Scalability:
===
store key-value data in storage nodes.
on demand change need to add or remove storage nodes. partition data over the nodes in the system to distribute the load across all nodes.

Eg: send request to node number is modulus of the hashed value with the number of nodes m
inefficient, like on removal we need to move the data to next node which is costly and can cause high latency

how to copy data efficiently?

• Consistent hashing:
effective way to manage the load over the set of nodes

we consider that we have a conceptual ring of hashes from 0 to n−1, where n is the number of available hash values.
We use each node’s ID, calculate its hash, and map it to the ring. We apply the same process to requests.
Each request is completed by the next node that it finds by moving in the clockwise direction in the ring.

Whenever a new node is added to the ring, the immediate next node is affected. It has to share its data with the newly added node while other nodes are unaffected. It’s easy to scale since we’re able to keep changes to our nodes minimal. This is because only a small portion of overall keys need to move. The hashes are randomly distributed, so we expect the load of requests to be random and distributed evenly on average on the ring.

Consider previous node N1, next node N2. We add N3 in between. N2 shares the keys from N1 to N3 to N3.

as nodes join or leave, it ensures that a minimal number of keys need to move. However, the request load isn’t equally divided in practice resulting in hotspots. means non-uniform load distribution has increased load on a single server.

	• Use virtual nodes:
		ensure a more evenly distributed load across the nodes.
		Instead of applying a single hash function, we’ll apply multiple hash functions(like 3) onto the same key, but for requests still one hash function.
		Advantages:
			• If a node fails or does routine maintenance, the workload is uniformly distributed over other nodes. For each newly accessible node, the other nodes receive nearly equal load when it comes back online or is added to the system.
			• It’s up to each node to decide how many virtual nodes it’s responsible for, considering the heterogeneity of the physical infrastructure. For example, if a node has roughly double the computational capacity as compared to the others, it can take more load using additional hash functions.


Data replication for hagh availability:
===
• Primary-secondary approach:
one of the storage areas is primary, and other storage areas are secondary
secondary replicates its data from the primary.
primary serves the write requests while the secondary serves read requests. After writing, there’s a lag for replication.

if the primary goes down, we can’t write into the storage, and it becomes a single point of failure. And we need to upgrade a secondary to a primary. The availability of write will suffer as we won’t allow writes during the switch-over time.

So, it doesn’t include the ability to always write.

• Peer-to-peer approach:
all involved storage areas are primary, and they replicate the data to stay updated.
Both read and write are allowed on all nodes.
Usually, it’s inefficient and costly to replicate in all n nodes. Instead, 3 or 5 is a common choice for the number of storage nodes to be replicated, which is configured per instance of the key-value store.

using this for replication to achieve durability and high availability in Key-value store.

A coordinator node is assigned the key “K.” It’s also responsible for replicating the keys to n−1 successors on the ring (clockwise). These lists of successor virtual nodes are called preference lists. To avoid putting replicas on the same physical nodes, the preference list can skip those virtual nodes whose physical node is already in the list.

we opt for asynchronous replication, it allows us to do speedy writes to the nodes. which doesn't effect availability like synchronous waiting for all acknowledgements.

In the context of the CAP theorem, key-value stores can either be consistent or be available when there are network partitions.
For key-value stores, we prefer availability over consistency. It means if the two storage nodes lost connection for replication, they would keep on handling the requests sent to them, and when the connection is restored, they’ll sync up. In the disconnected phase, it’s highly possible for the nodes to be inconsistent. So, we need to resolve such "conflicts". Handling inconsistencies using the versioning of our data.


Versioning Data and Achieving Configurability:
===
Data Versioning:
Resolving the conflicts among these divergent histories is essential and critical for consistency purposes.

To handle inconsistency, we need to maintain causality between the events. We can do this using the timestamps and update all conflicting values with the value of the latest request. But time isn’t reliable in a distributed system, so we can’t use it as a deciding factor.

Another approach to maintaining causality effectively is by using vector clocks. A "vector clock" is a list of (node, counter) pairs. There’s a single vector clock for every version of an object. If two objects have different vector clocks, we’re able to tell whether they’re causally related or not (more on this in a bit). Unless one of the two changes is reconciled, the two are deemed at odds.

Modify the API design:
we can decide if two events are causally related or not using a vector clock value. For this, we need information about which node performed the operation before and what its vector clock value was. This is the context of an operation.

get(key)
We return an object or a collection of conflicting objects along with a context.
The context holds encoded metadata about the object, including details such as the object’s version.

put(key, context, value)
The function finds the node where the value should be placed on the basis of the key and stores the value associated with it.
The context is returned by the system after the get operation. If we have a list of objects in context that raises a conflict, we’ll ask the client to resolve it.

To update an object in the key-value store, the client must give the context. We determine version information using a vector clock by supplying the context from a previous read operation. If the key-value store has access to several branches, it provides all objects at the leaf nodes, together with their respective version information in context, when processing a read request. Reconciling disparate versions and merging them into a single new version is considered an update.

Note: This process of resolving conflicts is comparable to how it’s done in Git.

<Vector clock usage example>

Compromise with vector clocks limitations:
The size of vector clocks may increase if multiple servers write to the same object simultaneously. It’s unlikely to happen in practice because writes are typically handled by one of the top n nodes in a preference list.

It’s a hassle to store and maintain such a long version history.
We can limit the size of the vector clock in these situations.
We employ a clock truncation strategy to store a timestamp with each (node, counter) pair to show when the data item was last updated by the node. Vector clock pairs are purged when the number of (node, counter) pairs exceeds a predetermined threshold (let’s say 10).
Because the descendant linkages can’t be precisely calculated, this truncation approach can lead to a lack of efficiency in reconciliation.

The get and put operations:
===
Every node can handle the get (read) and put (write) operations in our system. A node handling a read or write operation is known as a coordinator. The coordinator is the first among the top n nodes in the preference list.

There can be two ways for a client to select a node:
	• We route the request to a generic load balancer.
	• We use a partition-aware client library that routes requests directly to the appropriate coordinator nodes.
Both approaches have their benefits. The client isn’t linked to the code in the first approach, whereas "lower latency is achievable in the second". The latency is lower due to the reduced number of hops because the client can directly go to a specific server.

Let’s make our service "configurable" by having an ability where we can control the trade-offs between availability, consistency, cost-effectiveness, and performance. We can use a consistency protocol similar to those used in "quorum systems".

Let’s take an example. Say n in the top n of the preference list is equal to 3. This means three copies of the data need to be maintained. We assume that nodes are placed in a ring. Say A, B, C, D, and E is the clockwise order of the nodes in that ring. If the write function is performed on node A, then the copies of that data will be placed on B and C. This is because B and C are the next nodes we find while moving in a clockwise direction of the ring.

Usage of r and w:
r means the minimum number of nodes that need to be part of a successful read operation, while 
w is the minimum number of nodes involved in a successful write operation, "r-w nodes gets updated asynchronously"
we’ll use a quorum-like system by setting r+w>n.

Value Effects on Reads and Writes:
n	r	w	Description
3	2	1	It won't be allowed as it violates our constraint r + w > n.
3	2	2	It will be allowed as it fulfills constraints.
3	3	1	It will provide speedy writes and slower reads since readers need to go to all n replicas for a value.
3	1	3	It will provide speedy reads from any node but slow writes since we now need to write to all n nodes synchronously.

In this model, the latency of a get operation is decided by the slowest of the r replicas. The reason is that for the larger value of r, we focus more on availability and compromise consistency.

The coordinator produces the vector clock for the new version and writes the new version locally upon receiving a put() request for a key. The coordinator sends n highest-ranking nodes with the updated version and a new vector clock. We consider a write successful if at least w−1 nodes respond. Remember that the coordinator writes to itself first, so we get w writes in total.

Requests for a get() operation are made to the n highest-ranked reachable nodes in a preference list for a key. They wait for r answers before returning the results to the client. Coordinators return all dataset versions that they regard as unrelated if they get several datasets from the same source (divergent histories that need reconciliation). The conflicting versions are then merged, and the resulting key’s value is rewritten to override the previous versions.


Enable Fault Tolerance and Failure Detection:
===
• Handle temporary failures:
===
Typically, distributed systems use a quorum-based approach to handle failures. A quorum is the minimum number of votes required for a distributed transaction to proceed with an operation.
If a server is part of the consensus and is down, then we can’t perform the required operation. It affects the availability and durability of our system.

We’ll use a sloppy quorum instead of strict quorum membership, where the first n healthy nodes from the preference list handle all read and write operations. The n healthy nodes may not always be the first n nodes discovered when moving clockwise in the consistent hash ring.

This approach is called a "hinted handoff". Using it, we can ensure that reads and writes are fulfilled if a node faces temporary failure.

A minimal churn in system membership and transient node failures are ideal for hinted handoff. However, hinted replicas may become unavailable before being restored to the originating replica node in certain circumstances.

Note: A highly available storage system must handle data center failure due to power outages, cooling failures, network failures, or natural disasters. For this, we should ensure replication across the data centers. So, if one data center is down, we can recover it from the other.

• Handle permanent failures:
===
we should keep our replicas synchronized to make our system more durable. We need to speed up the detection of inconsistencies between replicas and reduce the quantity of transferred data.

In a "Merkle tree", the values of individual keys are hashed and used as the leaves of the tree. There are hashes of their children in the parent nodes higher up the tree. Each branch of the Merkle tree can be verified independently without the need to download the complete tree or the entire dataset. While checking for inconsistencies across copies, Merkle trees reduce the amount of data that must be exchanged. There’s no need for synchronization if, for example, the hash values of two trees’ roots are the same and their leaf nodes are also the same. Until the process reaches the tree leaves, the hosts can identify the keys that are out of sync when the nodes exchange the hash values of children.
The Merkle tree is a mechanism to implement anti-entropy, which means to keep all the data consistent. It reduces data transmission for synchronization and the number of discs accessed during the anti-entropy process.

Merkle trees are being used in Google code base while building the application to detect the changes. (ref: YouTube)

Anti-entropy with Merkle trees:
Each node keeps a distinct Merkle tree for the range of keys that it hosts for each virtual node. The nodes can determine if the keys in a given range are correct. The root of the Merkle tree corresponding to the common key ranges is exchanged between two nodes.

We’ll make the following comparison:
1. Compare the hashes of the root node of Merkle trees.
2. Do not proceed if they’re the same.
3. Traverse left and right children using recursion. The nodes identify whether or not they have any differences and perform the necessary synchronization.

The advantage of using Merkle trees is that each branch of the Merkle tree can be examined independently without requiring nodes to download the tree or the complete dataset. It reduces the quantity of data that must be exchanged for synchronization and the number of disc accesses that are required during the anti-entropy procedure.

The disadvantage is that when a node joins or departs the system, the tree’s hashes are recalculated because multiple key ranges are affected.

We want our nodes to detect the failure of other nodes in the ring, so let’s see how we can add it to our proposed design.

Promote membership in the ring to detect failures:
===
Planned commissioning and decommissioning of nodes results in membership changes. These changes form history. They’re recorded persistently on the storage for each node and reconciled among the ring members using a gossip protocol.
A "gossip-based protocol" also maintains an eventually consistent view of membership. When two nodes randomly choose one another as their peer, both nodes can efficiently synchronize their persisted membership histories.

Node A processes the request. Its token set has B and E in it
Node A gossips membership information to Node B and E after few requests

Keeping in mind our consistent hashing approach, the gossip-based protocol can fail.
Two virtual nodes of a same node, both nodes consider themselves to be part of the ring and won’t be aware that they’re the same server. If any change is made, it will keep on updating itself, which is wrong. This is called "logical partitioning".

We can make a few nodes play the role of seeds to avoid logical partitions. We can define a set of nodes as seeds via a configuration service. This set of nodes is known to all the working nodes since they can eventually reconcile their membership with a seed. So, logical partitions are pretty rare.

The gossip-based protocol works when all the nodes in the ring are connected in a single graph (i.e., have one connected component in the graph). That implies that there is a path from any node to any other node (possibly via different intermediaries). Different issues such as high churn (coming and going of nodes), issues with virtual node to physical node mappings, etc. can create a situation that is the same as if the real network had partitioned some nodes from the rest and now updates from one set won’t reach to the other.
Therefore just having a gossip protocol in itself is not sufficient for proper information dissemination; keeping the topology in a good, connected state is also necessary.

Decentralized failure detection protocols use a gossip-based protocol that allows each node to learn about the addition or removal of other nodes. The join and leave methods of the arriving or leaving nodes explicitly notify the other nodes about the permanent node additions and removals. The individual nodes detect temporary node failures when they fail to communicate with another node. If a node fails to communicate to any of the nodes present in its token set for the authorized time, then it communicates to the administrators that the node is dead.




Section: 5• Content Delivery Network (CDN):
=====
If millions of users worldwide use our data-intensive applications, and our service is deployed in a single data center to serve the users’ requests, what possible problems can arise?

Problems:
• High latency: The user-perceived latency will be high due to the physical distance from the serving data center. User-perceived latency has many components, such as transmission delays (a function of available bandwidth), propagation delays (a function of distance), queuing delays (a function of network congestion), and nodal processing delays. Therefore, data transmission over a large distance results in higher latency. Real-time applications require a latency below 200 milliseconds (ms) in general. For the Voice over Internet Protocol (VoIP), latency should not be more than 150 ms, whereas video streaming applications cannot tolerate a latency above a few seconds.

Note: According to one of the readings taken on December 21, 2021, the average latency from US East (N. Virginia) to US West (N. California) was 62.9 ms. Across continents—for example, from the US East (N. Virginia) to Africa (Cape Town)—was 225.63 ms. This is two-way latency, known as round-trip latency.

• Data-intensive applications: Data-intensive applications require transferring large traffic. Over a longer distance, this could be a problem due to the network path stretching through different kinds of ISPs. Because of some smaller Path message transmission unit (Path MTU, largest data unit that can traverse from source to destination without the need for splitting) links, the throughput of applications on the network might be reduced. Similarly, different portions of the network path might have different congestion characteristics. The problem multiplies as the number of users grows because the origin servers will have to provide the data individually to each user. That is, the primary data center will need to send out a lot of redundant data when multiple clients ask for it. However, applications that use streaming services are both data-intensive and dynamic in nature.

Note: According to a survey, 78% of the United States consumers use streaming services, which is an increase of 25% in five years(2015 to 2021).

• Scarcity of data center resources: Important data center resources like computational capacity and bandwidth become a limitation when the number of users of a service increases significantly. Services engaging millions of users simultaneously need scaling. Even if scaling is achieved in a single data center, it can still suffer from becoming a single point of failure when the data center goes offline due to natural calamity or connectivity issues with the Internet.

Note: According to one study, YouTube, Netflix, and Amazon Prime collectively generated 80% of Internet traffic in 2020. Circa 2016, the CDN provider Akamai served 15% to 30% of web traffic (about 30 terabits per second). For 90% of Internet users, Akamai was just one hop away. Therefore, we have strong reasons to optimize the delivery and consumption of this data without making the Internet core a bottleneck.

Introduction:
group of geographically distributed proxy servers.
A proxy server is an intermediate server between a client and the origin server.
The proxy servers are placed on the network edge(zone where a device or local network interfaces with the Internet).
reducing latency and saving bandwidth
stores two types of data: static and dynamic

Note: Various streaming protocols are used to deliver dynamic content by the CDN providers. For example, CDNsun uses the Real-time Messaging Protocol (RTMP), HTTP Live Streaming (HLS), Real-time Streaming Protocol (RTSP), and many more to deliver dynamic content.

Eg: Akamai, StackPath, Cloudflare, Rackspace, Amazon CloudFront, and Google Cloud CDN.

Does a CDN cache all content from the origin server?
Not likely. A CDN caches a considerable portion of the content depending on its capabilities, and it mostly caches the static content.
It also depends on the size of the content. For example, it might be possible for Netflix to store more than 90% of its movies in the CDN, while this might not be feasible for a service like YouTube due to the humongous volume of content.

Requirements:
Functional requirements:
	• Retrieve: Depending upon the type of CDN models(Push or Pull), a CDN should be able to retrieve content from the origin servers.
	• Request: Content delivery from the proxy server is made upon the user’s request. CDN proxy servers should be able to respond to each user’s request in this regard.
	• Deliver: In the case of the push model, the origin servers should be able to send the content to the CDN proxy servers.
	• Search: The CDN should be able to execute a search against a user query for cached or otherwise stored content within the CDN infrastructure.
	• Update: In most cases, content comes from the origin server, but if we run a script in a CDN, the CDN should be able to update the content within peer CDN proxy servers in a PoP(A Point of Presence (PoP) is a physical place that allows two or more networks or devices to communicate with each other. Typically, each CDN PoP has a large number of cache servers.).
	• Delete: Depending upon the type of content (static or dynamic), it should be possible to delete cached entries from the CDN servers after a certain period.
Non-functional requirements
	• Performance: Minimizing latency is one of the core missions of a CDN. The proposed design should have the minimum possible latency.
	• Availability: CDNs are expected to be available at all times because of their effectiveness. Availability includes protection against attacks like DDoS.
	• Scalability: An increasing number of users will request content from CDNs. Our proposed CDN design should be able to scale horizontally as the requirements increase.
	• Reliability and security: Our CDN design should ensure no single point of failure. Apart from failures, the designed CDN must reliably handle massive traffic loads. Furthermore, CDNs should provide protection to hosted content from various attacks.

Building blocks we will use:
	• DNS is the service that maps human-friendly CDN domain names to machine-readable IP addresses. This IP address will take the users to the specified proxy server.
	• Load balancers distribute millions of requests among the operational proxy servers.

Design of a CDN:
===
CDN components:
	• Clients: End users use various clients, like browsers, smartphones, and other devices, to request content from the CDN.
	• Routing system: The routing system directs clients to the nearest CDN facility. To do that effectively, this component receives input from various systems to understand where content is placed, how many requests are made for particular content, the load a particular set of servers is handling, and the URI (Uniform Resource Identifier) namespace of various contents.
	• Scrubber servers: Scrubber servers are used to separate the good traffic from malicious traffic and protect against well-known attacks, like DDoS. Scrubber servers are generally used only when an attack is detected. In that case, the traffic is scrubbed or cleaned and then routed to the target destination.
	• Proxy servers: The proxy or edge proxy servers serve the content from RAM to the users. Proxy servers store hot data in RAM, though they can store cold data in SSD or hard drive as well. These servers also provide accounting information and receive content from the distribution system.
	• Distribution system: The distribution system is responsible for distributing content to all the edge proxy servers to different CDN facilities. This system uses the Internet and intelligent broadcast-like approaches to distribute content across the active edge proxy servers.
	• Origin servers: The CDN infrastructure facilitates users with data received from the origin servers. The origin servers serve any unavailable data at the CDN to clients. Origin servers will use appropriate stores to keep content and other mapping metadata. Though, we won’t discuss the internal architecture of origin infrastructure here.
	• Management system: The management systems are important in CDNs from a business and managerial aspect where resource usage and statistics are constantly observed. This component measures important metrics, like latency, downtime, packet loss, server load, and so on. For third-party CDNs, accounting information can also be used for billing purposes.
Workflow:
	1. The origin servers provide the URI namespace delegation of all objects cached in the CDN to the request routing system.
	2. The origin server publishes the content to the distribution system responsible for data distribution across the active edge proxy servers.
	3. The distribution system distributes the content among the proxy servers and provides feedback to the request routing system. This feedback is helpful in optimizing the selection of the nearest proxy server for a requesting client. This feedback contains information about which content is cached on which proxy server to route traffic to relevant proxy servers.
	4. The client requests the routing system for a suitable proxy server from the request routing system.
	5. The request routing system returns the IP address of an appropriate proxy server.
	6. The client request routes through the scrubber servers for security reasons.
	7. The scrubber server forwards good traffic to the edge proxy server.
	8. The edge proxy server serves the client request and periodically forwards accounting information to the management system. The management system updates the origin servers and sends feedback to the routing system about the statistics and detail of the content. However, the request is routed to the origin servers if the content isn’t available in the proxy servers. It’s also possible to have a hierarchy of proxy servers if the content isn’t found in the edge proxy servers. For such cases, the request gets forwarded to the parent proxy servers.
API Design:
	• Retrieve (proxy server to origin server)
	retrieveContent(proxyserver_id, content_type, content_version, description)
		gives a response in a JSON file, which contains the text, content types, links to the images or videos in the content, and so on.
	• Deliver (origin server to proxy servers through the distribution system)
	deliverContent(origin_id, server_list, content_type, content_version, description)
	• Request (clients to proxy servers)
	requestContent(user_id, content_type, description)
	• Search (proxy server to peer proxy servers)
		Although the content is first searched locally at the proxy server, the proxy servers can also probe requested content in the peer proxy servers in the same PoP(In the context of Content Delivery Networks (CDNs), a Point of Presence (PoP) is a strategically located data center or network access point that serves as a key node for content distribution and delivery. PoPs are an integral part of a CDN’s infrastructure and play a crucial role in optimizing the delivery of web content, including web pages, images, videos, and other digital assets, to end-users).
		This could flood the query to all proxy servers in a PoP. Alternatively, we can use a data store in the PoP to query the content, though proxy servers will need to maintain what content is available on which proxy server.
	searchContent(proxyserver_id, content_type, description)
	• Update (proxy server to peer proxy servers)
		update the specified content in the peer proxy servers in the PoP. It does so when specified isolated scripts run on the CDN to provide image resizing, video resolution conversion, security, and many more services. This type of scripting is known as serverless scripting.
	updateContent(proxyserver_id, content_type, description)
	• Delete
		will be discussing in Cache section.


In-depth Investigation of CDN: Part 1:
===
• Content caching strategies in CDN:
	Identifying content to cache is important in delivering up-to-date and popular web content.
	• Push CDN:
		Content gets sent automatically to the CDN proxy servers from the origin server in the push CDN model. The content delivery to the CDN proxy servers is the content provider’s responsibility.
		Appropriate for "static content" delivery, where the origin server decides which content to deliver to users using the CDN. The content is pushed to proxy servers in various locations according to the content’s popularity.
	• Pull CDN
		A CDN pulls the unavailable data from origin servers when requested by a user. The proxy servers keep the files for a specified amount of time and then remove them from the cache if they’re no longer requested to balance capacity and cost.
		When users request web content in the pull CDN model, the CDN itself is responsible for pulling the requested content from the origin server and serving it to the users.
		more suited for serving "dynamic content".

	push CDN scheme maintains more replicas than the pull CDN, thus improving availability.
	pull CDN is favored for frequently changing content and a high traffic load. Low storage consumption is one of the main benefits of the pull CDN.

	Note: Most content providers use both pull and push CDN caching approaches to get the benefits of both.

• Dynamic content caching optimization:
	Dynamic data can be generated using various parameters, certain dynamic content creation requires the execution of scripts that can be executed at proxy servers instead of running on the origin server.

	it’s useful to employ compression techniques as well.
	For example, Cloudflare uses Railgun to compress dynamic content.

	Edge Side Includes (ESI) markup language specifies where content was changed so that the rest of the web page content can be cached. It assembles dynamic content at the CDN edge server or client browser.
	ESI isn’t standardized yet by the World Wide Web Consortium (W3C), but many CDN providers use it.

	Note: Dynamic Adaptive Streaming over HTTP (DASH) uses a manifest file with URIs of the video with different resolutions so that the client can fetch whatever is appropriate as per prevailing network and end node conditions. Netflix uses a proprietary DASH version with a Byte-range in the URL for further content request and delivery optimization.

• Multi-tier CDN architecture:
	CDNs follow a tree-like structure to ease the data distribution process for the origin server.
	set of servers receives data from the parent nodes in the tree, which eventually receive data from the origin servers.
	The data is copied from the origin server to the proxy servers by following different paths in the tree.

	Whenever a new proxy server enters the tree of a CDN, it requests the control core(set of servers that manage the proxy servers within the PoP.), which maintains information on all the proxy servers in the CDN and provides initial content with the configuration data.

	"multi-layer cache" might be used to handle long-tail content

• Find the nearest proxy server to fetch the data:
	• Important factors that affect the proximity of the proxy server:
		• Network distance between the user and the proxy server is crucial.
			• length of the network path.
			• capacity (bandwidth) limits along the network path.
		• Requests load refers to the load a proxy server handles at any point in time. If a set of proxy servers are overloaded, the request routing system should forward the request to a location with a lesser load. This action balances out the proxy server load and, consequently, reduces the response latency.
	• DNS redirection: return another URI (instead of an IP) to the client
		client tries to resolve a name, authoritative DNS server provides another URL (for example, cdn.xyz.com). The client does another DNS resolution, and the CDN’s authoritative DNS provides an IP address of an appropriate CDN proxy server to fetch the required content.

		Note: The nearest proxy server doesn’t necessarily mean the one that’s geographically the closest. It could be, but it’s not only the geography that matters. Other factors like network distance, bandwidth, and traffic load already on that route also matter.

		Steps in DNS redirection approach:
			• it maps the clients to the appropriate network location.
			• it distributes the load over the proxy servers in that location to balance the load among the proxy servers.
		To shift a client from one machine in a cluster to another, the DNS replies at the second step are given with short TTLs so that the client repeats the resolution after a short while.
	• Anycast: routing methodology in which all the edge servers located in multiple locations share the same single IP address. It employs the Border Gateway Protocol (BGP, network-level protocol used by Internet edge routers to share routing and reachability information so that every node on the network, even if independent, is aware of the status of their closest network neighbors.) to route clients based on the Internet’s natural network flow. A CDN provider can use the anycast mechanism so that clients are directed to the nearest proxy servers for content.
	• Client multiplexing:  involves sending a client a list of candidate servers. The client then chooses one server from the list to send the request to. This approach is "inefficient" because the client lacks the overall information to choose the most suitable server for their request. This may result in sending requests to an already-loaded server and experiencing higher access latency.
	• HTTP redirection: simplest of all approaches. With this scheme, the client requests content from the origin server. The origin server responds with an HTTP protocol to redirect the user via a URL of the content.

