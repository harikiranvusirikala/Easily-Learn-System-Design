Structure of Notes:
===
1. Introduction:
System Design Interviews
Introduction
Abstractions
Non-functional system characteristics
Back-of-the-envelope calculations


Section: System Design interviews:
=====
System Design interviews are open to discussion and involve multiple possible solutions that can be re-iterated.

ability to approach a problem, think critically, and make trade-offs.
understanding the problem, breaking it down, and finding the most optimal solution.

figure out the requirements and map them onto the computational components and the high-level communication protocols that connect these subsystems.

The final answer doesn’t matter. What matters is the process and the journey that a good applicant takes the interviewer through.

Best practices:
ask the right questions to solidify the requirements
need to scope the problem
engage the interviewer

how a design might evolve over time as some aspect of the system increases by some order of magnitude

There’s no single correct approach or solution to a design problem.
A lot is predicated on the assumptions we make.

our responsibility: provide fault tolerance at the design level

Theory: mostly from distributed systems - https://www.educative.io/courses/distributed-systems-practitioners

• Robustness (the ability to maintain operations during a crisis)
• Scalability
• Availability
• Performance
• Extensibility
• Resiliency (the ability to return to normal operations over an acceptable period of time post-disruption)

interviewer evaluates the candidate’s ability to discuss the different aspects of the system and assess the solution based on the requirements that might evolve during the conversation.



Key Concepts to Prepare:
=====
1. Fundamental concepts in System Design interview:
===
CAP theorem - availability or consistency under-network partitions when network components fail.

• PACELC theorem: CAP theorem and without partitions, latency or consistency.

• If Network (P)artitioning happens then
	• Choose from (A)vailability Or (C)onsistency
• (E)lse
	• Choose from (L)atency Or (C)onsistency

PC/EC system include BigTable and HBase
PA/EL system include Dynamo and Cassandra
PA/EC system include MongoDB

• Heartbeat: all servers periodically send a heartbeat message

• AJAX polling/HTTP short-polling: client request, server respone. Empty response on no data.

• HTTP long-polling/hanging GET: hold till data available instead of empty, client immediately re-requests information.

• WebSockets: connection through WebSocket handshake. server & client bidrectional data exchange.

• Server-sent events (SSEs): long-term connection, server send data to client, but client required the use another technology or protocol to send data.


2. Fundamentals of distributed system:
===
• Data durability and consistency: We must understand the differences and impacts of storage solution failure and corruption rates in read-write processes.

• Replication: key to unlocking data durability and consistency. It deals with backing up data but also with repeating processes at scale.

• Partitioning/sharding: partitions divide data across different nodes within our system. As replication distributes data across nodes, partitioning distributes processes across nodes, reducing the reliance on pure replication.

• Consensus: Consider each node in different location of world, can all be synchronized. This is consensus.
all the nodes need to agree, which will prevent faulty processes from running and ensure consistency and replication of data and processes across the system.

• Distributed transactions: Once we’ve achieved consensus, now transactions from applications need to be committed across databases, with fault checks performed by each involved resource. Two-way and three-way communication to read, write, and commit are shared across participant nodes.


3. The architecture of large-scale web applications:
===
• N-tier applications: processing happens in different layers(called tiers) some on client, server, and other server in one application.
Understanding how these tiers interact and processes they are responsible is part of System Design for the web.

• HTTP and REST: HTTP is the sole API on which the entire internet runs.
REST is a set of design principles to directly interact with the API that is HTTP, allowing efficient, scalable systems with components isolated from each other’s assumptions.

• DNS and load balancing: Helps in not making a server overload, while other is idle.
Routing client requests to the right server, the right tier where processing happens, helps ensure system stability.

• Caching: A cache makes our most frequently requested data and applications accessible to most users at high speeds.
The questions for our web application are what needs to be stored in the cache, how we direct traffic to the cache, and what happens when we don’t have what we want in the cache.

• Stream processing: Stream processing applies uniform processes to the data stream.
If an application has continuous, consistent data passing through it, then stream processing allows efficient use of local resources within the application.


4. Design of large-scale distributed systems:
===
Once we know the basics of "distributed systems" and "web architecture", it is time to apply this learning and design real-world systems.
• https://www.educative.io/courses/distributed-systems-practitioners
• https://www.educative.io/courses/software-architecture-in-applications

Finding and optimizing potential solutions to these problems will give us the tools to approach the System Design interview with confidence.
=====


Resources:
===
This course
Technical blogs/System Design interview articles
We can study these blogs to gain insight into the company’s challenges or problems and the changes it made in the design to cope with them.

https://engineering.fb.com/
https://research.fb.com/
https://aws.amazon.com/blogs/architecture/
https://www.amazon.science/blog
https://netflixtechblog.com/
https://research.google/
https://quoraengineering.quora.com/
https://eng.uber.com/
https://databricks.com/blog/category/engineering
https://medium.com/@Pinterest_Engineering
https://medium.com/blackrock-engineering
https://eng.lyft.com/
https://engineering.salesforce.com/

https://www.educative.io/blog/google-system-design-interview-questions
https://www.educative.io/blog/microsoft-system-design-interview
https://www.educative.io/blog/netflix-system-design-interview-questions
https://www.educative.io/blog/amazon-system-design-interview
https://www.educative.io/blog/meta-system-design-interview

https://www.educative.io/blog/category/system-design
https://www.educative.io/blog/system-design-interview-questions

We should start with high-level stuff because the low-level details will automatically come up.

https://www.educative.io/mock-interview


Do's: Strategize, then divide and conquer:
===
1• Ask refining questions
prioritize the main features by asking the interviewer refining questions
	• clients direct requirements(functional requirements) example, the ability to send messages in near real-time to friends.
	• indirect requirements(non-functional requirements (NFRs)) example, messaging service performance shouldn’t degrade with increasing user load.

2• Handle data
identify and understand data and its characteristics in order to look for "appropriate data storage systems and data processing components" for the system design.
	• What’s the size of the data right now?
	• At what rate is the data expected to grow over time?
	• How will the data be consumed by other subsystems or end users?
	• Is the data read-heavy or write-heavy?
	• Do we need strict consistency of data, or will eventual consistency work?
	• What’s the durability target of the data?
	• What privacy and regulatory requirements do we require for storing or transmitting user data?

3• Discuss the components
figuring out which components we’ll use, where they’ll be placed, and how they’ll interact with each other.
like will a conventional database work, or should we use a NoSQL database?
Front-end components, load balancers, caches, databases, firewalls, and CDNs are just some examples of system components.

4• Discuss trade-offs
	• Different components have different pros and cons. We’ll need to carefully weigh what works for us.
	• Different choices have different costs in terms of money and technical complexity. We need to efficiently utilize our resources.
	• Every design has its weaknesses. As designers, we should be aware of all of them, and we should have a follow-up plan to tackle them.
We should point out weaknesses in our design to our interviewer and explain why we haven’t tackled them yet.

Don't:
===
	• Don’t write code in a system design interview.
	• Don’t start building without a plan.
	• Don’t work in silence.
	• Don’t describe numbers without reason. We have to frame it.
	• If we don’t know something, we don’t paper over it, and we don’t pretend to know it.
Note: If an interviewer asks us to design a system we haven’t heard of, we should just be honest and tell them so. The interviewer will either explain it to us or they might change the question.


Steps to build large-scale distributed systems:
===
1. Determine system requirements and constraints: Initially, identify what the system needs to do (functional requirements) and any limitations it must operate within (non-functional requirements and constraints).

2. Recognize components: Based on the requirements, select the appropriate components, technologies, or APIs that will form part of the system.

3. Generate design: Create a design that integrates the selected components in a way that meets the various requirements.

4. Identify shortcomings in the initial design: Review the initial design to find any limitations or areas for improvement.

5. Discuss trade-offs and improve iteratively: Finally, optimize the design by considering different trade-offs and making iterative improvements.


1. What is a system design interview?
interactive nature, importance of discussing trade-offs, different from coding interviews due to their higher level of abstraction, the process matters more than the final answer, and they often include questions about how a design might evolve over time.
Additionally, the theory of system design comes from the domain of distributed systems.

2. How to prepare for success?
Your approach to preparation, including going through courses, reading technical blogs, understanding how popular applications work and building side projects and participating in mock interviews.
Focusing on trade-offs rather than mechanics is also crucial.

3. How to perform well?
You’ve outlined a good strategy for performing well by engaging the interviewer, refining requirements, designing components, and discussing trade-offs. Remember, it’s important to avoid looking unoriginal and to have a plan to attack the problem.
Identifying and understanding data and its characteristics can help in choosing the right systems and components.
Moreover, acknowledging that there’s no one correct answer to a design problem and every design has its weaknesses is vital.


Section: Introduction:
=====
System design is the process of defining components and their integration, APIs, and data models to build large-scale systems that meet a specified set of functional and non-functional requirements.

• Reliable systems handle faults, failures, and errors.
• Effective systems meet all user needs and business requirements.
• Maintainable systems are flexible and easy to scale up or down. The ability to add new features also comes under the umbrella of maintainability.

16 crucial building blocks

recommend two iterations—first, where we do our best to come up with a design (that takes about 80 percent of our time), and a second iteration for improvements.



Section: Abstractions:
=====
Abstraction is the art of obfuscating details that we don’t need. It allows us to concentrate on the big picture.

Transactions is a database abstraction that hides many problematic outcomes when concurrent users are reading, writing, or mutating the data and gives a simple interface of commit, in case of success, or abort, in case of failure.

Abstractions in distributed systems help engineers simplify their work and relieve them of the burden of dealing with the underlying complexity of the distributed systems.

Network Abstractions: Remote Procedure Calls(RPCs)
RPC method is similar to calling a local procedure, except that the called procedure is usually executed in a different process and on a different computer.


Consistency Models:
===
we can look into the consistency guarantees provided by S3 to decide whether to use it or not. We no need to worry how it handles the consistency, that is abstraction here.

There is a difference between consistency in ACID properties and consistency in the CAP theorem.

• Eventual consistency: ensures that all the replicas converge on a final value after a finite time and when no more writes are coming in.
All nodes gives same value only, but the old one at the time just someone updating.
Ensures high availability.
Eg: DNS, Cassandra(NoSQL database)

• Causal consistency: works by categorizing operations into dependent(causally-related operations) and independent operations.
preserves the order of the causally-related operations.

x=a
b=x+5
y=b

read of x, write of y are causally-related
used in commenting system, like replies to a comment(we want to display comments after the comment it replies to)

• Sequential consistency: preserves the ordering specified by each client’s program.
Eg: social networking applications(don't care post order, but still anticipate single friend’s posts to appear in the correct order in which they were created)

• Strict consistency aka linearizability: ensures that a read request from any replicas will get the latest write value.
Once the client receives the acknowledgment that the write operation has been performed, other clients can read that value.

Linearizability is challenging to achieve in a distributed system. Some of the reasons for such challenges are variable network delays and failures.

Usually, synchronous replication is one of the ingredients for achieving strong consistency, though it in itself is not sufficient.
We might need consensus algorithms such as Paxos and Raft to achieve strong consistency.

Linearizability affects the system’s availability, which is why it’s not always used.
Applications with strong consistency requirements use techniques like quorum-based replication to increase the system’s availability.

* Trade-off: Application programmers have to compromise performance and availability if they use services with strong consistency models.


Failure Models:
===
• Fail-stop: a node halts permanently, other nodes can still detect that node by communicating with it.
• Crash: a node halts silently, other nodes can’t detect that the node has stopped working.
• Omission failures: node fails to send or receive messages
send omission failure - node fails to respond to the incoming request
receive omission failure - node fails to receive the request and thus can’t acknowledge it
• Temporal failures: node generates correct results, but is too late to be useful. This failure could be due to bad algorithms, a bad design strategy, or a loss of synchronization between the processor clocks.
• Byzantine failures: node exhibits random behavior like transmitting arbitrary messages at arbitrary times, producing wrong results, or stopping midway.
This mostly happens due to an attack by a malicious entity or a software bug.
most challenging type of failure to deal with.



Section: Non-functional System Characteristics:
=====
1• Availability - time loss: percentage of time a system accepts requests and responds to clients.
We measure availability as a number of nines.
99.9% (3 nines)		8.76 hours/year		43.8 minutes/month 		10.1 minutes/week downtime

2• Reliability - frequency and impact of failures: probability that the service will perform its functions for a specified time.
how well a system performs its intended operations (functional requirements).
R measured - mean time between failures (MTBF) and mean time to repair (MTTR).
We strive for a higher MTBF value and a lower MTTR value.
we use MTTF instead of MTBF for those cases where a failed component is replaced due to irreparable problems.
A is a function of R. This means that the value of R can change independently, and the value of A depends on R.

3• Scalability: ability of a system to handle an increasing amount of workload without compromising performance.

Workload types:
	• Request workload: 			number of requests served by the system.
	• Data/storage workload: 	amount of data stored by the system.
Dimensions:
	• Size scalability: system can scale simply by adding user or resource.
	• Administrative scalability: system’s ability to manage an increasing number of users or organizations without a significant increase in administrative effort or complexity.
	• Geographical scalability: system can readily service a broad geographical region, as well as a smaller one.
Different approaches of scalability:
	• Vertical scalability—scaling up: prroviding additional capabilities (for example, additional CPUs or RAM) to an existing device.
	allows us to expand our present hardware or software capacity, but we can only grow it to the limitations of our server.
	The dollar cost of vertical scaling is usually high because we might need exotic components to scale up.
	• Horizontal scalability—scaling out: increasing the number of machines in the network.
	We use "commodity nodes"(inexpensive compared to specialized or high-end hardware) for this purpose because of their attractive dollar-cost benefits.
	The catch here is that we need to build a system such that many nodes could collectively work as if we had a single, huge server.

4• Maintainability: keeping the system up and running by finding and fixing bugs, adding new functionalities, keeping the system’s platform updated, and ensuring smooth system operations.
	• Operability: This is the ease with which we can ensure the system’s smooth operational running under normal circumstances and achieve normal conditions under a fault.
	• Lucidity: This refers to the simplicity of the code. The simpler the code base, the easier it is to understand and maintain it, and vice versa.
	• Modifiability: This is the capability of the system to integrate modified, new, and unforeseen features without any hassle.

probability that the service will restore its functions within a specified time of fault occurrence.
M measured - mean time to repair (MTTR)

Maintainability refers to time-to-repair, whereas
Reliability refers to both time-to-repair and the time-to-failure

5• Fault Tolerance: refers to a system’s ability to execute persistently even if one or more of its components fail.
• Replication: We can swap out failed nodes with healthy ones and a failed data store with its replica.

* Trade-off: When a system needs strong consistency, we can synchronously update data in replicas. However, this reduces the availability of the system.
We can also asynchronously update data in replicas when we can tolerate eventual consistency, resulting in stale reads until all replicas converge.
We compromise either on availability or on consistency under failures—a reality that is outlined in the CAP theorem.

• Checkpointing: technique that saves the system’s state in stable storage for later retrieval in case of failures due to errors or service disruptions.
	• Consistent state: all the individual processes of a system have a consistent view of the shared state or sequence of events that have occurred in a system.
	 • All updates to data that were completed before the checkpoint are saved. Any updates to data that were in progress are rolled back as if they didn’t initiate.
	 • Checkpoints include all the messages that have been sent or received up until the checkpoint. No messages are in transit (in-flight) to avoid cases of missing messages.
	 • Relationships and dependencies between system components and their states match what would be expected during normal operation.
	• Inconsistent state: This is a state where there are discrepancies in the saved state of different processes of a system.
	checkpoints across different processes are not coherent and coordinated.



Section: Back-of-the-envelope calculations (BOTECs): involve swift, approximate, and simplified estimations or computations typically done on paper or, figuratively, on the back of an envelope.
=====

Types of data center servers:
===
1• Web servers: first point of contact after load balancers.
mostly serve static content to the client

2• Application servers: run the core application software and business logic.
primarily provide dynamic content

3• Storage servers
mainly include structured (for example, SQL) and nonstructured (NoSQL) data management systems.
YouTube uses the following data stores:
1. Blob storage: This is used for its encoded videos.
2. Temporary processing queue storage: This can hold a few hundred hours of video content uploaded daily to YouTube for processing.
3. Bigtable(NoSQL, key-value store, large amount of structured and semi-structured data, by Google): This is a specialized storage used for storing a large number of thumbnails of videos.
4. Relational database management system (RDBMS): This is for users’ and videos’ metadata (comments, likes, user channels, and so on).
Other data stores are still used for analytics, for example, Hadoop’s HDFS.

Organizations also require servers for services like configuration, monitoring, load balancing, analytics, accounting, caching, and so on.

• Typical Server:
Processor 					Intel Xeon (Sapphire Rapids 8488C)
Number of cores			64 cores
RAM									256 GB
Cache (L3)					112.5 MB
Storage capacity		16 TB

• Important Latencies:
Component														Time (nanoseconds)
L1 cache reference									0.9 
L2 cache reference									2.8 
L3 cache reference									12.9 
Main memory reference								100 
Compress 1KB with Snzip							3,000 (3 microseconds)
Read 1 MB sequentially from memory	9,000 (9 microseconds)
Read 1 MB sequentially from SSD			200,000 (200 microseconds)
Round trip within same datacenter		500,000 (500 microseconds)
Read 1 MB sequentially from SSD with speed ~1GB/sec SSD			1,000,000 (1 milliseconds)
Disk seek														4,000,000 (4 milliseconds)
Read 1 MB sequentially from disk		2,000,000 (2 milliseconds)
Send packet SF->NYC									71,000,000 (71 milliseconds)

Remember the order of magnitude difference between different components and operations is more important than remembering the exact numbers.
For example, we should know that doing IO-bound work (for example, reading 1 MB data sequentially from the SSD disk - 200 micro) is two orders of magnitude slower than CPU-bound work (for example, compressing 1 KB data as snzip - 3 micro).

• Throughput - measured as queries per second (QPS)
Important Rates:
QPS handled by MySQL						1000						needs to go through query planning
QPS handled by key-value store	10,000					simpler API (put and get)
QPS handled by cache server			100,000–1 M			read and write operations

For real projects, initial designs use BOTECs similar to the ones we use in a system design interview.

Transaction Processing Performance Council Benchmark C(TPC-C) is a benchmark to compare the performance of online transaction processing systems.

• Request types:
	• CPU-bound requests: processor - X
	• Memory-bound requests: memory - 10X
	• IO-bound requests: IO subsystem (such as disks or the network) - 100X

BOTECs for making quick, high-level estimates and decisions in the early stages.

• Resource Estimations:
	• Servers needed = No.of request/s / request a server can handle
		Peak capacity
			• Improving the RPS of a server
			• Improving over the peak load assumption
		Cost of server = $3.54/h ($2.6/h with 1y term saving plan)
	• Storage requirements
	• Bandwidth requirements = ((daily incoming & outgoing data)Bytes / 86400) * 8bits Gbps(Gigabits per second)

Plausibility test: evaluate assumptions, estimates, or outcomes seem reasonable and realistic

Poisson distribution: gives probability of an outcome, number of events occurring with in a time

Pareto principle: assumption that 80% of our peak traffic occurs within 20% of the time

In 2023, 20 TB disks are readily available.
Many data centers of an organization are connected with high speed networks such as 1Tbps.




Structure of Notes:
===
2. Building Blocks:
Introduction
DNS
Load balancers
Databases
Key-value store
CDN
Sequencer
Distributed monitoring
Monitoring server-side errors
Monitoring client-side errors
Distributed cache
Distributed messaging queue
Pub-sub system
Rate limiter
Blob store
Distributed search
Distributed logging
Distributed task scheduler
Sharded counters
Concluding building blocks



Section: Building Blocks: Introduction:
=====
Many of the building blocks we discuss are also available for actual use in the public clouds, such as Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP).

