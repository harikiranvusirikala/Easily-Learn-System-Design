1• Domain Name System(DNS)
	• Name servers: respond to users’ queries.
	• Resource records(RR): form of domain name to IP address mappings.
	• Caching:
	• Hierarchy: allows DNS to be highly scalable

	• DNS hierarchy
		• Types
			• DNS resolver / local or default servers: initiate the querying sequence and forward requests to the other DNS name servers
			• Root-level name servers: receive requests from local servers, return a list of top-level domain (TLD) servers that hold the IP addresses of the .xyz domain
			• Top-level domain (TLD) name servers: hold the IP addresses of authoritative name servers.
			• Authoritative name servers: organization’s DNS name servers that provide the IP addresses of the web or application servers
		• Ways
			• Iterative: local server requests the root, TLD, and the authoritative servers for the IP address
			• Recursive
	• DNS as a distributed system
		• Highly scalable
		• Reliable
			• Caching: in the browser, operating systems, local name server within the user’s network, or the ISP’s DNS resolvers
			• Server replication
			• Protocol: many rely on unreliable User Datagram Protocol (UDP) which is faster
		• Consistent: eventual consistency as many reads compared to writes
	• Test it out
		• The nslookup output
		• The dig output

2• Load Balancers
	• Capabilities:
		• Scalability: upscaling or downscaling of capacity of the application/service transparent to the end users
		• Availability: hide faults and failures of server
		• Performance: forward requests to servers with a lesser load, not only improves performance but also improves resource utilization
	• Placing load balancers:
		• between user and web servers/application gateway
		• between web servers and application servers that run the business/application logic
		• between application servers and database servers
	• Services offered:
		• Health checking: heartbeat protocol to monitor the health and, therefore, reliability of end-servers. 
		• TLS termination(TLS/SSL offloading): reduce the burden on end-servers by handling TLS termination with the client.
		• Predictive analytics: predict traffic patterns through analytics performed over traffic passing through them or using statistics of traffic obtained over time.
		• Reduced human intervention: automation of handling failures.
		• Service discovery: clients’ requests are forwarded to appropriate hosting servers by inquiring about the service registry.
		• Security: mitigating attacks like denial-of-service (DoS) at different layers of the OSI model (layers 3, 4, and 7).

	• Global server load balancing (GSLB): distribution of traffic load across multiple geographical regions.
		• Load balancing in DNS: uses round-robin to reorders the list of IP addresses
			• Limitations:
				• Different ISPs have a different number of users.
				• round-robin load-balancing algorithm doesn’t consider any end-server crashes
		• Need for local load balancers:
			• DNS limitations:
				• The small size of the DNS packet (512 Bytes) isn’t enough to include all possible IP addresses of the servers.
				• There’s limited control over the client’s behavior. Clients may select arbitrarily from the received set of IP addresses. Some of the received IP addresses may belong to busy data centers.
				• Clients can’t determine the closest address to establish a connection with.
				• In case of failures, recovery can be slow through DNS because of the caching mechanism, especially when TTL values are longer.
	• Local load balancing: achieved within a data center. focuses on improving efficiency and better resource utilization of the hosting servers in a data center. requests seamlessly connect to the LB that uses a virtual IP address (VIP - no physical machine, group of machines will use the same address)

	• Ways of Global traffic management (GTM):
		• GTM through ADCs: Some Application Delivery Controllers (ADCs) implement GSLB. have a real-time view of the hosting servers and forward requests based on the health and capacity of the data center.
		• GTM through DNS:
	
	• Algorithms:
		• Round-robin scheduling
		• Weighted round-robin
		• Least connections
		• Least response time
		• IP hash
		• URL hash
		other algorithms also, like randomized or weighted least connections algorithms

		• Algorithm types:
			• Static algorithms: don’t consider the changing state of the servers
			• Dynamic algorithms: maintain state by communicating with the server, so more complicated
		• Session maintenance through LBs:
			• Stateful load balancing: maintaining a state of the sessions established between clients and hosting servers, uses state in algorithm, increase complexity and limit scalability
			• Stateless load balancing: maintains no state and is, therefore, faster and lightweight, use consistent hashing to make forwarding decisions.
	• Types:
		• Layer 4 load balancers(stateless?): on transport protocols like TCP and UDP. maintain connection/session with the clients and ensure that the same (TCP/UDP) communication ends up being forwarded to the same back-end server.
		smart in terms of inspection.
		• Layer 7 load balancers(stateful?): on the data of application layer protocols. possible to make application-aware forwarding decisions based on HTTP headers, URLs, cookies, and other application-specific data—for example, user ID. Apart from performing TLS termination, these LBs can take responsibilities like rate limiting users, HTTP routing, and header rewriting.
		faster in terms of processing.
	• Deployment:
		• Tier-0 and Tier-1 LBs: DNS as the tier-0 LB, Equal-cost Multi-path (ECMP) routers are the tier-1 LBs, which balances the load among the load balancers themselves.
		ECMP routers, equal routing priority, play a vital role in the horizontal scalability of the higher-tier LBs.
		• Tier-2 LBs: include layer 4 load balancers, enables a smooth transition from tier 1 to tier 3 in case of failures
		• Tier-3 LBs: Layer 7 LBs, maintain state, perform health monitoring of servers at HTTP level, reduces the burden on end-servers by handling low-level details like TCP-congestion control protocols, the discovery of Path MTU (maximum transmission unit), the difference in application protocol between client and back-end servers, and so on. actual load balancing between back-end servers.

		Direct Routing (DR) or Direct Server Return (DSR), which is a method where the back-end server bypasses certain load balancers in the response path.
	• Implementation:
		• Hardware load balancers: work as stand-alone devices and are quite expensive
		• Software load balancers: flexibility, programmability, and cost-effectiveness. implemented on commodity hardware. provide predictive analysis that can help prepare for future traffic patterns.
		• Cloud load balancers: Load Balancers as a Service (LBaaS)
			may not necessarily replace a local on-premise load balancing facility, but they can perform global traffic management between different zones
			ease of use, management, metered cost, flexibility in terms of usage, auditing, and monitoring services to improve business decisions

3• Databases
	• Limitations of file storage
		• We can’t offer concurrent management to separate users accessing the storage files from different locations.
		• We can’t grant different access rights to different users.
		• How will the system scale and be available when adding thousands of entries?
		• How will we search content for different users in a short time?
	• Advantages:
		• Managing large data: A large amount of data can be easily handled with a database
		• Retrieving accurate data (data consistency): can retrieve accurate data whenever we want.
		• Easy updation: using data manipulation language (DML).
		• Security: only allows authorized users to access data.
		• Data integrity: using different constraints for data.
		• Availability: can be replicated (using data replication) on different servers, which can be concurrently updated.
		• Scalability: divided (using data partitioning) to manage the load on a single node.

	• Types:
		1• SQL (relational databases)
			* Trade-off: Traditional databases are vertically scalable. Vertical scaling has limits.
			• ACID:
				• Atomicity: A "transaction is considered an atomic unit". Therefore, either all the statements within a transaction will successfully execute, or none of them will execute.
				• Consistency: At any given time, the database should be in a consistent state, and it should remain in a consistent state after every transaction.
				• Isolation: In the case of multiple transactions running concurrently, they shouldn’t be affected by each other. The final state of the database should be the same as the transactions that were executed sequentially.
				• Durability: The system should guarantee that completed transactions will survive permanently in the database even in system failure events.
			• Features:
				• Flexibility: using Data definition language (DDL) for modification, modify schema while other queries are happening and the database server is running.
				• Reduced redundancy: using foreign keys. This process is called "normalization" and has the additional benefit of removing an inconsistent dependency.
				• Concurrency:
					handled through transactional access to the data
					transaction is considered an atomic operation, so it also works in error handling to either roll back or commit a transaction on successful execution
				• Integration: aggregating data from multiple sources. all applications easily access each other’s data while the concurrency control measures handle the access of multiple applications.
				• Backup and disaster recovery: export and import operations make backup and restoration easier.
					cloud-based relational databases perform "continuous mirroring" to avoid loss of data and make the restoration process easier and quicker.
			• Drawback:
				• Impedance mismatch:
					difference between the relational model(can’t store a structure or a list) and the in-memory data structures(complex data structure can be stored)
					To make the complex structures compatible with the relations, we would need a translation of the data in light of relational algebra.
		2• NoSQL (non-relational databases)
			dynamic schema.
			used in applications that require a large volume of semi-structured and unstructured data, low latency, and flexible data models
			• Characteristics
				• Simple design: doesn’t require dealing with the impedance mismatch
				• Horizontal scaling:
					ability to run databases on a large cluster
					stored in one document instead of multiple tables over nodes
					often spread data across multiple nodes and balance data and queries across nodes automatically
				• Availability: transparently replace without any application disruption, during node failure. support data replication
				• Support for unstructured and semi-structured data:
					work with data that doesn’t have schema at the time of database configuration or data writes
					For example, document databases are structureless; they allow documents (JSON, XML, BSON, and so on) to have different fields.
				•	Cost:
					open source and freely available, but RDBMSs expensive.
					use clusters of cheap commodity servers, but RDBMSs costly proprietary hardware and storage systems.
			• Types:
				• Key-value database: - Eg: Amazon DynamoDB, Redis, and Memcached DB
						use key-value methods like hash tables to store data in key-value pairs
					For "session-oriented applications", such as web applications, store users’ data in the main memory or in a database during a session.
					This data may include user profile information, recommendations, targeted promotions, discounts, and more.
					A unique ID (a key) is assigned to each user’s session for easy access and storage.
				• Document database: - Eg: MongoDB and Google Cloud Firestore
						designed to store and retrieve documents in formats like XML, JSON, BSON, and so on.
						documents are composed of a hierarchical tree data structure that can include maps, collections, and scalar values.
						Documents in this type of database may have varying structures and data.
						An entity required for the application is stored as a single document in such applications.
					For "e-commerce applications", a product has thousands of attributes, which is unfeasible to store in a relational database due to its impact on the reading performance.
					For "content management applications", such as blogs and video platforms.
				• Graph database: - Eg: Neo4J, OrientDB, and InfiniteGraph.
					use the graph data structure to store data, where nodes represent entities, and edges show relationships between entities.
					gives interesting patterns between the nodes.
					allows us to store the data once and then interpret it differently based on relationships.
					Graph data is kept in store files for persistent storage. Each of the files contains data for a specific part of the graph, such as nodes, links, properties, and so on.
					The focus is to store data and pave the way to drive analyses and decisions based on relationships.
					For "social applications" and provide interesting facts and figures among different kinds of users and their activities.
					For various applications, such as "data regulation and privacy, machine learning research, financial services-based applications", and many more.
				• Columnar database: - Eg: HBase, Hypertable, and Amazon Redshift.
					store data in columns instead of rows.
					enable access to all entries in the database column quickly and efficiently.
					efficient for a large number of aggregation and data analytics queries.
					It drastically reduces the disk I/O requirements and the amount of data required to load from the disk.
					For applications related to "financial institutions", there’s a need to sum the financial transaction over a period of time. Columnar databases make this operation quicker by just reading the column for the amount of money, ignoring other attributes of customers.
			• Drawbacks:
				• Lack of standardization:
					doesn’t follow any specific standard, like how relational databases follow relational algebra.
					Porting applications from one type of NoSQL database to another might be a challenge.
				• Consistency:
					* Trade-off: between consistency and availability when failures can happen.
					We won’t have strong data integrity, like primary and referential integrities in a relational database.
					Data might not be strongly consistent but slowly converging using a weak model like eventual consistency.

	• Data Replication
		• Complexities: data consistent, failed replica, synchronously or asynchronously(lag), and concurrent writes
		• Ways
			• Synchronous replication
			• Asynchronous replication
		* Trade-off: between data consistency and availability when different components of the system can fail.
		• Models
			1• Single leader/primary-secondary replication - read-heavy
				• Methods
					• Statement-based replication (SBR) - MySQL - statements to log file then secondayr. NOW()?
					• Write-ahead log (WAL) shipping - PostgreSQL and Oracle - transactional logs(in disk)
					• Logical (row-based) replication - PostgreSQL and MySQL - captured individual rows
			2• Multi-leader replication
				• Conflict: Since all the primary nodes concurrently deal with the write requests, they may modify the same data, which can create a conflict between them
				• Handle Conflicts:
					• Conflict avoidance - for particular verify all write same leader
					• Last-write-wins
					• Custom logic
				• Topologies
					• circular topology - drawback if one of the nodes fails
					• star topology - drawback if one of the nodes fails
					• all-to-all topology - most common/used
			3• Peer-to-peer or leaderless replication - Cassandra
				• Quorums - to solve inconsistency due to concurrent writes
					w+r>n (update at least w nodes, must read r node, among n nodes)

