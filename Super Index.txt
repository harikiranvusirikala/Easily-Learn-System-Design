1• Domain Name System(DNS)
	• Name servers: respond to users’ queries.
	• Resource records(RR): form of domain name to IP address mappings.
	• Caching:
	• Hierarchy: allows DNS to be highly scalable

	• DNS hierarchy
		• Types
			• DNS resolver / local or default servers: initiate the querying sequence and forward requests to the other DNS name servers
			• Root-level name servers: receive requests from local servers, return a list of top-level domain (TLD) servers that hold the IP addresses of the .xyz domain
			• Top-level domain (TLD) name servers: hold the IP addresses of authoritative name servers.
			• Authoritative name servers: organization’s DNS name servers that provide the IP addresses of the web or application servers
		• Ways
			• Iterative: local server requests the root, TLD, and the authoritative servers for the IP address
			• Recursive
	• DNS as a distributed system
		• Highly scalable
		• Reliable
			• Caching: in the browser, operating systems, local name server within the user’s network, or the ISP’s DNS resolvers
			• Server replication
			• Protocol: many rely on unreliable User Datagram Protocol (UDP) which is faster
		• Consistent: eventual consistency as many reads compared to writes
	• Test it out
		• The nslookup output
		• The dig output

2• Load Balancers
	• Capabilities:
		• Scalability: upscaling or downscaling of capacity of the application/service transparent to the end users
		• Availability: hide faults and failures of server
		• Performance: forward requests to servers with a lesser load, not only improves performance but also improves resource utilization
	• Placing load balancers:
		• between user and web servers/application gateway
		• between web servers and application servers that run the business/application logic
		• between application servers and database servers
	• Services offered:
		• Health checking: heartbeat protocol to monitor the health and, therefore, reliability of end-servers. 
		• TLS termination(TLS/SSL offloading): reduce the burden on end-servers by handling TLS termination with the client.
		• Predictive analytics: predict traffic patterns through analytics performed over traffic passing through them or using statistics of traffic obtained over time.
		• Reduced human intervention: automation of handling failures.
		• Service discovery: clients’ requests are forwarded to appropriate hosting servers by inquiring about the service registry.
		• Security: mitigating attacks like denial-of-service (DoS) at different layers of the OSI model (layers 3, 4, and 7).

	• Global server load balancing (GSLB): distribution of traffic load across multiple geographical regions.
		• Load balancing in DNS: uses round-robin to reorders the list of IP addresses
			• Limitations:
				• Different ISPs have a different number of users.
				• round-robin load-balancing algorithm doesn’t consider any end-server crashes
		• Need for local load balancers:
			• DNS limitations:
				• The small size of the DNS packet (512 Bytes) isn’t enough to include all possible IP addresses of the servers.
				• There’s limited control over the client’s behavior. Clients may select arbitrarily from the received set of IP addresses. Some of the received IP addresses may belong to busy data centers.
				• Clients can’t determine the closest address to establish a connection with.
				• In case of failures, recovery can be slow through DNS because of the caching mechanism, especially when TTL values are longer.
	• Local load balancing: achieved within a data center. focuses on improving efficiency and better resource utilization of the hosting servers in a data center. requests seamlessly connect to the LB that uses a virtual IP address (VIP - no physical machine, group of machines will use the same address)

	• Ways of Global traffic management (GTM):
		• GTM through ADCs: Some Application Delivery Controllers (ADCs) implement GSLB. have a real-time view of the hosting servers and forward requests based on the health and capacity of the data center.
		• GTM through DNS:
	
	• Algorithms:
		• Round-robin scheduling
		• Weighted round-robin
		• Least connections
		• Least response time
		• IP hash
		• URL hash
		other algorithms also, like randomized or weighted least connections algorithms

		• Algorithm types:
			• Static algorithms: don’t consider the changing state of the servers
			• Dynamic algorithms: maintain state by communicating with the server, so more complicated
		• Session maintenance through LBs:
			• Stateful load balancing: maintaining a state of the sessions established between clients and hosting servers, uses state in algorithm, increase complexity and limit scalability
			• Stateless load balancing: maintains no state and is, therefore, faster and lightweight, use consistent hashing to make forwarding decisions.
	• Types:
		• Layer 4 load balancers(stateless?): on transport protocols like TCP and UDP. maintain connection/session with the clients and ensure that the same (TCP/UDP) communication ends up being forwarded to the same back-end server.
		smart in terms of inspection.
		• Layer 7 load balancers(stateful?): on the data of application layer protocols. possible to make application-aware forwarding decisions based on HTTP headers, URLs, cookies, and other application-specific data—for example, user ID. Apart from performing TLS termination, these LBs can take responsibilities like rate limiting users, HTTP routing, and header rewriting.
		faster in terms of processing.
	• Deployment:
		• Tier-0 and Tier-1 LBs: DNS as the tier-0 LB, Equal-cost Multi-path (ECMP) routers are the tier-1 LBs, which balances the load among the load balancers themselves.
		ECMP routers, equal routing priority, play a vital role in the horizontal scalability of the higher-tier LBs.
		• Tier-2 LBs: include layer 4 load balancers, enables a smooth transition from tier 1 to tier 3 in case of failures
		• Tier-3 LBs: Layer 7 LBs, maintain state, perform health monitoring of servers at HTTP level, reduces the burden on end-servers by handling low-level details like TCP-congestion control protocols, the discovery of Path MTU (maximum transmission unit), the difference in application protocol between client and back-end servers, and so on. actual load balancing between back-end servers.

		Direct Routing (DR) or Direct Server Return (DSR), which is a method where the back-end server bypasses certain load balancers in the response path.
	• Implementation:
		• Hardware load balancers: work as stand-alone devices and are quite expensive
		• Software load balancers: flexibility, programmability, and cost-effectiveness. implemented on commodity hardware. provide predictive analysis that can help prepare for future traffic patterns.
		• Cloud load balancers: Load Balancers as a Service (LBaaS)
			may not necessarily replace a local on-premise load balancing facility, but they can perform global traffic management between different zones
			ease of use, management, metered cost, flexibility in terms of usage, auditing, and monitoring services to improve business decisions

3• Databases
	• Limitations of file storage
		• We can’t offer concurrent management to separate users accessing the storage files from different locations.
		• We can’t grant different access rights to different users.
		• How will the system scale and be available when adding thousands of entries?
		• How will we search content for different users in a short time?
	• Advantages:
		• Managing large data: A large amount of data can be easily handled with a database
		• Retrieving accurate data (data consistency): can retrieve accurate data whenever we want.
		• Easy updation: using data manipulation language (DML).
		• Security: only allows authorized users to access data.
		• Data integrity: using different constraints for data.
		• Availability: can be replicated (using data replication) on different servers, which can be concurrently updated.
		• Scalability: divided (using data partitioning) to manage the load on a single node.

	• Types:
		1• SQL (relational databases)
			* Trade-off: Traditional databases are vertically scalable. Vertical scaling has limits.
			• ACID:
				• Atomicity: A "transaction is considered an atomic unit". Therefore, either all the statements within a transaction will successfully execute, or none of them will execute.
				• Consistency: At any given time, the database should be in a consistent state, and it should remain in a consistent state after every transaction.
				• Isolation: In the case of multiple transactions running concurrently, they shouldn’t be affected by each other. The final state of the database should be the same as the transactions that were executed sequentially.
				• Durability: The system should guarantee that completed transactions will survive permanently in the database even in system failure events.
			• Features:
				• Flexibility: using Data definition language (DDL) for modification, modify schema while other queries are happening and the database server is running.
				• Reduced redundancy: using foreign keys. This process is called "normalization" and has the additional benefit of removing an inconsistent dependency.
				• Concurrency:
					handled through transactional access to the data
					transaction is considered an atomic operation, so it also works in error handling to either roll back or commit a transaction on successful execution
				• Integration: aggregating data from multiple sources. all applications easily access each other’s data while the concurrency control measures handle the access of multiple applications.
				• Backup and disaster recovery: export and import operations make backup and restoration easier.
					cloud-based relational databases perform "continuous mirroring" to avoid loss of data and make the restoration process easier and quicker.
			• Drawback:
				• Impedance mismatch:
					difference between the relational model(can’t store a structure or a list) and the in-memory data structures(complex data structure can be stored)
					To make the complex structures compatible with the relations, we would need a translation of the data in light of relational algebra.
		2• NoSQL (non-relational databases)
			dynamic schema.
			used in applications that require a large volume of semi-structured and unstructured data, low latency, and flexible data models
			• Characteristics
				• Simple design: doesn’t require dealing with the impedance mismatch
				• Horizontal scaling:
					ability to run databases on a large cluster
					stored in one document instead of multiple tables over nodes
					often spread data across multiple nodes and balance data and queries across nodes automatically
				• Availability: transparently replace without any application disruption, during node failure. support data replication
				• Support for unstructured and semi-structured data:
					work with data that doesn’t have schema at the time of database configuration or data writes
					For example, document databases are structureless; they allow documents (JSON, XML, BSON, and so on) to have different fields.
				•	Cost:
					open source and freely available, but RDBMSs expensive.
					use clusters of cheap commodity servers, but RDBMSs costly proprietary hardware and storage systems.
			• Types:
				• Key-value database: - Eg: Amazon DynamoDB, Redis, and Memcached DB
						use key-value methods like hash tables to store data in key-value pairs
					For "session-oriented applications", such as web applications, store users’ data in the main memory or in a database during a session.
					This data may include user profile information, recommendations, targeted promotions, discounts, and more.
					A unique ID (a key) is assigned to each user’s session for easy access and storage.
				• Document database: - Eg: MongoDB and Google Cloud Firestore
						designed to store and retrieve documents in formats like XML, JSON, BSON, and so on.
						documents are composed of a hierarchical tree data structure that can include maps, collections, and scalar values.
						Documents in this type of database may have varying structures and data.
						An entity required for the application is stored as a single document in such applications.
					For "e-commerce applications", a product has thousands of attributes, which is unfeasible to store in a relational database due to its impact on the reading performance.
					For "content management applications", such as blogs and video platforms.
				• Graph database: - Eg: Neo4J, OrientDB, and InfiniteGraph.
					use the graph data structure to store data, where nodes represent entities, and edges show relationships between entities.
					gives interesting patterns between the nodes.
					allows us to store the data once and then interpret it differently based on relationships.
					Graph data is kept in store files for persistent storage. Each of the files contains data for a specific part of the graph, such as nodes, links, properties, and so on.
					The focus is to store data and pave the way to drive analyses and decisions based on relationships.
					For "social applications" and provide interesting facts and figures among different kinds of users and their activities.
					For various applications, such as "data regulation and privacy, machine learning research, financial services-based applications", and many more.
				• Columnar database: - Eg: HBase, Hypertable, and Amazon Redshift.
					store data in columns instead of rows.
					enable access to all entries in the database column quickly and efficiently.
					efficient for a large number of aggregation and data analytics queries.
					It drastically reduces the disk I/O requirements and the amount of data required to load from the disk.
					For applications related to "financial institutions", there’s a need to sum the financial transaction over a period of time. Columnar databases make this operation quicker by just reading the column for the amount of money, ignoring other attributes of customers.
			• Drawbacks:
				• Lack of standardization:
					doesn’t follow any specific standard, like how relational databases follow relational algebra.
					Porting applications from one type of NoSQL database to another might be a challenge.
				• Consistency:
					* Trade-off: between consistency and availability when failures can happen.
					We won’t have strong data integrity, like primary and referential integrities in a relational database.
					Data might not be strongly consistent but slowly converging using a weak model like eventual consistency.

	• Data Replication
		• Complexities: data consistent, failed replica, synchronously or asynchronously(lag), and concurrent writes
		• Ways
			• Synchronous replication
			• Asynchronous replication
		* Trade-off: between data consistency and availability when different components of the system can fail.
		• Models
			1• Single leader/primary-secondary replication - read-heavy
				• Methods
					• Statement-based replication (SBR) - MySQL - statements to log file then secondayr. NOW()?
					• Write-ahead log (WAL) shipping - PostgreSQL and Oracle - transactional logs(in disk)
					• Logical (row-based) replication - PostgreSQL and MySQL - captured individual rows
			2• Multi-leader replication
				• Conflict: Since all the primary nodes concurrently deal with the write requests, they may modify the same data, which can create a conflict between them
				• Handle Conflicts:
					• Conflict avoidance - for particular verify all write same leader
					• Last-write-wins
					• Custom logic
				• Topologies
					• circular topology - drawback if one of the nodes fails
					• star topology - drawback if one of the nodes fails
					• all-to-all topology - most common/used
			3• Peer-to-peer or leaderless replication - Cassandra
				• Quorums - to solve inconsistency due to concurrent writes
					w+r>n (update at least w nodes, must read r node, among n nodes)

	• Data Partitioning (or sharding)
		hotspots - highly congested partitions
		• Ways
			• Vertical sharding
			• Horizontal sharding
				• Strategies
					• Key-range based sharding: each partition is assigned a continuous range of keys.
					* Trade-off between an impact on increased storage and locating the desired shards efficiently.
					• Hash-based sharding - Keys are uniformly distributed across the nodes
						No of shards the database should be distributed in = Database size / Size of a single shard
					• Consistent hashing: assigns each server or item in a distributed hash table a place on an abstract circle, called a ring, irrespective of the number of servers in the table
						easy to scale horizontally
		• Strategies for rebalancing the partitions
			• Avoid hash mod n
			• Fixed number of partitions - Elasticsearch, Riak
			• Dynamic partitioning - HBase and MongoDB
			• Partition proportionally to nodes - Cassandra and Ketama

		• Request routing:
			• Allow the clients to request any node in the network. If that node doesn’t contain the requested data, it forwards that request to the node that does contain the related data.
			• The second approach contains a routing tier. All the requests are first forwarded to the routing tier, and it determines which node to connect to fulfill the request.
			• The clients already have the information related to partitioning and which partition is connected to which node. So, they can directly contact the node that contains the data they need.
		• ZooKeeper: keeps track of all the mappings in the network, and each node connects to ZooKeeper for the information.
			HBase, Kafka and SolrCloud

	• * Trade-offs
		• Centralized database:
			• Advantages:
				• Data maintenance, such as updating and taking backups is easy.
				• provide stronger consistency and ACID transactions than distributed databases.
				• provide a much simpler programming model for the end programmers as compared to distributed databases.
				• more efficient for businesses that have a small amount of data to store that can reside on a single node.
			• Disadvantages:
				• can slow down, causing high latency for end users, when the number of queries per second accessing is approaching single-node limits.
				• has a single point of failure. Because of this, its probability of not being accessible is much higher.
		• Distributed database (having sharding):
			• Advantages:
				• fast and easy to access data because data is retrieved from the nearest database shard or the one frequently used.
				• Data with different levels of distribution transparency can be stored in separate places.
				• Intensive transactions consisting of queries can be divided into multiple optimized subqueries, which can be processed in a parallel fashion.
			• Disadvantages:
				• Sometimes, data is required from multiple sites(locations), which takes more time than expected.
				• Relations are partitioned vertically or horizontally among different nodes. Therefore, operations such as joins need to reconstruct complete relations by carefully fetching data. These operations can become much more expensive and complex.
				• difficult to maintain consistency of data across sites, and it requires extra measures.
				• Updations and backups take time to synchronize data.

			Total communication time T = a + v/b (a = Total access delay, v = Total data volume, b = Data rate)

4• Key-value Store
	keep size KB to MB, larger go to blob
	For "storing user sessions" in a web application and "building NoSQL databases"
	Eg: bestseller lists, shopping carts, customer preferences, session management, sales rank, and product catalogs.
	• Design of a Key-value Store:
		• Functional requirements
			• Configurable service: control over the trade-offs between availability, consistency, cost-effectiveness, and performance
			• Ability to always write (when we picked “A” over “C” in the context of CAP): always have the ability to write into the key-value storage
			• Hardware heterogeneity: balancing workload distribution according to each server’s capacity
		• Non-functional requirements
			• Scalability
			• Fault tolerance
	• API design
		• get(key)
			when replication, locates replica associated with a specific key
			in eventual consistency, there might be more than one value returned
		• put(key, value)
			system automatically determines where data should be placed.
			system often keeps metadata about the stored object, which can include the version of the object.
	• Scalability
		add or remove storage nodes, copy data efficiently?
		• Consistent hashing
			conceptual ring of hashes from 0 to n−1
			For both node and request do hash and place/send in a place of ring
			Consider previous node N1, next node N2. We add N3 in between. N2 shares the keys from N1 to N3 to N3.
			• Use virtual nodes - for uniform load distribution, and no hotspots
				apply multiple hash functions(like 3) onto the same key, but for requests still one hash function
				• If a node fails or does routine maintenance, the workload is uniformly distributed over other nodes.
				• node to decide how many virtual nodes it’s responsible for, considering the heterogeneity of the physical infrastructure.
	• Data replication for high availability:
		• Primary-secondary approach: primary serves the write requests while the secondary serves read requests. After writing, there’s a lag for replication.
		• Peer-to-peer approach
			configure no of instances(3 or 5 commonly) it should replicate
			preference lists - lists of successor virtual nodes (skipping viritual nodes more than one of it's)
			we opt for asynchronous replication, it allows us to do speedy writes to the nodes. which doesn't effect availability
			we prefer availability over consistency
	• Data Versioning and Achieving Configurability:
		to resolve conflicts
		"vector clock" is a list of (node, counter) pairs.
		There’s a single vector clock for every version of an object.
		• Modify the API design:
			• get(key)
				return an object or a collection of conflicting objects along with a context.
				context holds encoded metadata about the object, including details such as the object’s version.
			• put(key, context, value)
				finds the node where the value should be placed on the basis of the key and stores.
				context is returned by the system after the get operation. If we have a list of objects in context that raises a conflict, we’ll ask the client to resolve it.
		• Vector clock usage example
		• Compromise with vector clocks limitations:
			size of vector clocks may increase if multiple servers write to the same object simultaneously.  It’s unlikely to happen in practice because writes are typically handled by one of the top n nodes in a preference list.
		• get and put operations
			Every node can handle the get (read) and put (write) operations in our system
			There can be two ways for a client to select a node:
				• We route the request to a generic load balancer.
				• We use a partition-aware client library that routes requests directly to the appropriate coordinator nodes.
			client isn’t linked to the code in the first approach, whereas "lower latency is achievable in the second". The latency is lower due to the reduced number of hops because the client can directly go to a specific server.
			consistency protocol - quorum systems - w+r>n
	• Enable Fault Tolerance and Failure Detection:
		• Handle temporary failures:
			hinted handoff approach - We’ll use a sloppy quorum instead of strict quorum membership, where the first n healthy nodes from the preference list handle all read and write operations. The n healthy nodes may not always be the first n nodes discovered when moving clockwise in the consistent hash ring.
		• Handle permanent failures:
			• Anti-entropy with Merkle trees:
				In a "Merkle tree", the values of individual keys are hashed and used as the leaves of the tree. There are hashes of their children in the parent nodes higher up the tree.
				• Advantage: each branch of the Merkle tree can be examined independently without requiring nodes to download the tree or the complete dataset. It reduces the quantity of data that must be exchanged for synchronization and the number of disc accesses that are required during the anti-entropy procedure.
				• Disadvantage: is that when a node joins or departs the system, the tree’s hashes are recalculated because multiple key ranges are affected.
		• Promote membership in the ring to detect failures:
			"gossip-based protocol" also maintains an eventually consistent view of membership. When two nodes randomly choose one another as their peer, both nodes can efficiently synchronize their persisted membership histories.
			We can make a few nodes play the role of seeds to avoid logical partitions(due to virtual gosip to another virtual node of same node).

